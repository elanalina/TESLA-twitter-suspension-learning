{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Spam Learning: Classifer\n",
    "## CSCE 670 Spring 2018, Course Project\n",
    "### By: Rose Lin (826009602)\n",
    "\n",
    "There will be series of notebooks outlining how we train our models. This one looks into [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the aggregated user data\n",
    "\n",
    "Our main data sources come from the [Bot Repository](https://botometer.iuni.iu.edu/bot-repository/datasets.html):\n",
    "\n",
    "* Varol-2017, which was released in 2017. It contains 2,573 user IDs crawled in April 2016. We repeatedly called Twitter API to crawl account and tweet information. Despite some suspended accounts, we were able to get information of 2,401 users. \n",
    "* cresci-2017, annotated by CrowdFlower contributors. We downloded the whole dataset and used the following labels: genuine, social_spambots_1, social_spambots_2, social_spambots_3, traditional_spambots_1, traditional_spambots_2, traditional_spambots_3, and traditional_spambots_4.\n",
    "\n",
    "Initially, our aggregated user dataset was imbalanced. We had ~7,000 labeled spammers and ~5,000 labeled legitimate users. To balance it out, we did not use oversampling/downsampling; rather, we utilized Twitter API again. Our crawler started from President Trump's [Twitter account](https://twitter.com/realDonaldTrump) and scraped his friends lists, his friends' following lists, and so on until we collected 2,000 rows of user data. We assume that President Trump is following real users, and his friends follow authentic accounts as well. In this way, we were able to gather a balanced user dataset with spammers:legitimate users ratio roughly to be 1:1. It is acknowledged that our aggregated dataset may subject to biases. If time permits, we will collect a larger dataset that covers as many groups as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the user data...\n",
      "Number of users: 14202\n",
      "Number of spammers: 7293\n",
      "Number of legitmate users: 7364\n"
     ]
    }
   ],
   "source": [
    "#Loading the aggregated data\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print \"Loading the user data...\"\n",
    "user = pd.read_csv(cwd+\"/all_users_balanced.csv\",sep=',',header='infer')\n",
    "print \"Number of users:\",user.id.nunique()\n",
    "print \"Number of spammers:\",len(user[user.user_type == 1])\n",
    "print \"Number of legitmate users:\",len(user[user.user_type == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors_enabled</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>created_at</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>follow_request_sent</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following</th>\n",
       "      <th>...</th>\n",
       "      <th>profile_text_color</th>\n",
       "      <th>profile_use_background_image</th>\n",
       "      <th>protected</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>url</th>\n",
       "      <th>user_type</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2014-04-19 14:46:19</td>\n",
       "      <td>Tue Mar 17 08:51:12 +0000 2009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>22</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>davideb66</td>\n",
       "      <td>1299</td>\n",
       "      <td>Rome</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-18 23:20:58</td>\n",
       "      <td>Sun Apr 19 14:38:04 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Autrice del libro #unavitatuttacurve dal 9 apr...</td>\n",
       "      <td>16358</td>\n",
       "      <td></td>\n",
       "      <td>12561</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>ElisaDospina</td>\n",
       "      <td>18665</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>http://t.co/ceK8TovxwI</td>\n",
       "      <td>1</td>\n",
       "      <td>-7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-13 23:21:54</td>\n",
       "      <td>Wed May 13 15:34:41 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Live Long and Prosper]</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>600</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>Vladimir65</td>\n",
       "      <td>22987</td>\n",
       "      <td>Rome</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-19 23:24:18</td>\n",
       "      <td>Wed Jul 15 12:55:03 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Cuasi Odontologa*♥,#Bipolar, #Sarcastica &amp; Som...</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>398</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3E4415</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>RafielaMorales</td>\n",
       "      <td>7975</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-25200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-11 23:22:23</td>\n",
       "      <td>Wed Aug 05 21:12:49 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>I shall rise from my own death, to avenge hers...</td>\n",
       "      <td>162</td>\n",
       "      <td></td>\n",
       "      <td>413</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>D67345</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>FabrizioC_c</td>\n",
       "      <td>20218</td>\n",
       "      <td>Rome</td>\n",
       "      <td>http://t.co/PK5F0JDKcy</td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors_enabled           crawled_at                      created_at  \\\n",
       "0                       2014-04-19 14:46:19  Tue Mar 17 08:51:12 +0000 2009   \n",
       "1                       2014-05-18 23:20:58  Sun Apr 19 14:38:04 +0000 2009   \n",
       "2                       2014-05-13 23:21:54  Wed May 13 15:34:41 +0000 2009   \n",
       "3                       2014-05-19 23:24:18  Wed Jul 15 12:55:03 +0000 2009   \n",
       "4                       2014-05-11 23:22:23  Wed Aug 05 21:12:49 +0000 2009   \n",
       "\n",
       "  default_profile default_profile_image  \\\n",
       "0             1.0                   1.0   \n",
       "1                                         \n",
       "2                                         \n",
       "3                                         \n",
       "4                                         \n",
       "\n",
       "                                         description  favourites_count  \\\n",
       "0                                                                    1   \n",
       "1  Autrice del libro #unavitatuttacurve dal 9 apr...             16358   \n",
       "2                            [Live Long and Prosper]                14   \n",
       "3  Cuasi Odontologa*♥,#Bipolar, #Sarcastica & Som...                11   \n",
       "4  I shall rise from my own death, to avenge hers...               162   \n",
       "\n",
       "  follow_request_sent  followers_count following   ...     profile_text_color  \\\n",
       "0                                   22             ...                 333333   \n",
       "1                                12561             ...                 333333   \n",
       "2                                  600             ...                 333333   \n",
       "3                                  398             ...                 3E4415   \n",
       "4                                  413             ...                 D67345   \n",
       "\n",
       "  profile_use_background_image  protected     screen_name statuses_count  \\\n",
       "0                          1.0                  davideb66           1299   \n",
       "1                          1.0               ElisaDospina          18665   \n",
       "2                          1.0                 Vladimir65          22987   \n",
       "3                          1.0             RafielaMorales           7975   \n",
       "4                          1.0                FabrizioC_c          20218   \n",
       "\n",
       "                    time_zone                     url user_type utc_offset  \\\n",
       "0                        Rome                                 1     7200.0   \n",
       "1                   Greenland  http://t.co/ceK8TovxwI         1    -7200.0   \n",
       "2                        Rome                                 1     7200.0   \n",
       "3  Pacific Time (US & Canada)                                 1   -25200.0   \n",
       "4                        Rome  http://t.co/PK5F0JDKcy         1     7200.0   \n",
       "\n",
       "  verified  \n",
       "0           \n",
       "1           \n",
       "2           \n",
       "3           \n",
       "4           \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the head\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mainly converted the json response from Twitter API into the dataframe, with two additional features:\n",
    "* crawled_at: the date a record was crawled. It will be used for account age computation.\n",
    "* user_type: 0 = normal users, 1 = spammers. It serves as a binary indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Based on the visualization results, we will consider the following account features:\n",
    "\n",
    "* Count of favorite tweets\n",
    "* Friends to follower ratio\n",
    "* Total status count\n",
    "* Default profile image\n",
    "* Default profile\n",
    "* Account ages\n",
    "* Username, count of characters\n",
    "* Username, count of numbers\n",
    "* Screen_name, count of characters\n",
    "* Screen_name, count of numbers\n",
    "* Length of description \n",
    "* Description text\n",
    "* Average tweet per day\n",
    "\n",
    "These features will be derived from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Create a new dataframe to store the result\n",
    "usert = pd.DataFrame()\n",
    "# add count of favorite tweets\n",
    "usert['favorite_count'] = user['favourites_count']\n",
    "# add friends to follower ratio\n",
    "usert['friends_to_followers'] = user['friends_count'] / user['followers_count']\n",
    "# add total status count\n",
    "usert['statuses_count'] = user['statuses_count']\n",
    "# add default profile image\n",
    "temp_df = pd.get_dummies(user['default_profile_image'])\n",
    "temp_df.columns = ['def_p_img_na','def_p_img_false','def_p_img_true']\n",
    "usert = pd.concat([usert, temp_df], axis=1)\n",
    "# add default profile\n",
    "temp_df = pd.get_dummies(user['default_profile'])\n",
    "temp_df.columns = ['def_p_na','def_p_false','def_p_true']\n",
    "usert = pd.concat([usert, temp_df], axis=1)\n",
    "# add account ages \n",
    "agedf = pd.to_datetime(user['crawled_at'])-pd.to_datetime(user['created_at'])\n",
    "usert['age'] = agedf.dt.days\n",
    "# add username, count of characters and letters\n",
    "for index, item in user['name'].iteritems():\n",
    "    letter = 0\n",
    "    num = 0\n",
    "    for c in item:\n",
    "        if c.isalpha():\n",
    "            letter += 1\n",
    "        elif c.isdigit():\n",
    "            num += 1\n",
    "    usert.loc[index,'name_letter'] = letter\n",
    "    usert.loc[index,'name_num'] = num\n",
    "# add screen name, count of characters and letters\n",
    "for index, item in user['screen_name'].iteritems():\n",
    "    letter = 0\n",
    "    num = 0\n",
    "    for c in item:\n",
    "        if c.isalpha():\n",
    "            letter += 1\n",
    "        elif c.isdigit():\n",
    "            num += 1\n",
    "    usert.loc[index,'screen_letter'] = letter\n",
    "    usert.loc[index,'screen_num'] = num\n",
    "# add len of description\n",
    "usert['des_len'] = pd.Series([len(d) for d in user['description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14657x21265 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 124464 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add description text (TFIDF)\n",
    "import re, string\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "des_text = tfidf_transformer.fit_transform(user['description'].tolist())\n",
    "des_text\n",
    "# because the description text is tooooo large, we won't add it to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   favorite_count  friends_to_followers  statuses_count  def_p_img_na  \\\n",
      "0               1              1.818182            1299             0   \n",
      "1           16358              0.274023           18665             1   \n",
      "2              14              1.258333           22987             1   \n",
      "3              11              0.879397            7975             1   \n",
      "4             162              0.980630           20218             1   \n",
      "\n",
      "   def_p_img_false  def_p_img_true  def_p_na  def_p_false  def_p_true   age  \\\n",
      "0                0               1         0            0           1  1859   \n",
      "1                0               0         1            0           0  1855   \n",
      "2                0               0         1            0           0  1826   \n",
      "3                0               0         1            0           0  1769   \n",
      "4                0               0         1            0           0  1740   \n",
      "\n",
      "   name_letter  name_num  screen_letter  screen_num  des_len  \\\n",
      "0         13.0       0.0            7.0         2.0        1   \n",
      "1         12.0       0.0           12.0         0.0      134   \n",
      "2         14.0       0.0            8.0         2.0       23   \n",
      "3         15.0       0.0           14.0         0.0      151   \n",
      "4          4.0       0.0           10.0         0.0       79   \n",
      "\n",
      "   avg_tweet_per_day  \n",
      "0           0.698763  \n",
      "1          10.061995  \n",
      "2          12.588719  \n",
      "3           4.508197  \n",
      "4          11.619540  \n",
      "\n",
      "       favorite_count  friends_to_followers  statuses_count  def_p_img_na  \\\n",
      "count    14657.000000          1.432600e+04    14657.000000  14657.000000   \n",
      "mean      2986.562257                   inf    12743.150304      0.678515   \n",
      "std      11686.509649                   NaN    34701.634675      0.467062   \n",
      "min          0.000000          0.000000e+00        0.000000      0.000000   \n",
      "25%          0.000000          5.604008e-01       65.000000      0.000000   \n",
      "50%         17.000000          1.064804e+00     1536.000000      1.000000   \n",
      "75%       1565.000000          4.549167e+00    10905.000000      1.000000   \n",
      "max     410844.000000                   inf   781664.000000      1.000000   \n",
      "\n",
      "       def_p_img_false  def_p_img_true      def_p_na   def_p_false  \\\n",
      "count     14657.000000    14657.000000  14657.000000  14657.000000   \n",
      "mean          0.310227        0.011257      0.582657      0.212254   \n",
      "std           0.462602        0.105506      0.493137      0.408917   \n",
      "min           0.000000        0.000000      0.000000      0.000000   \n",
      "25%           0.000000        0.000000      0.000000      0.000000   \n",
      "50%           0.000000        0.000000      1.000000      0.000000   \n",
      "75%           1.000000        0.000000      1.000000      0.000000   \n",
      "max           1.000000        1.000000      1.000000      1.000000   \n",
      "\n",
      "         def_p_true           age   name_letter      name_num  screen_letter  \\\n",
      "count  14657.000000  14657.000000  14657.000000  14657.000000   14657.000000   \n",
      "mean       0.205090   1235.965545     10.980214      0.027905      10.961998   \n",
      "std        0.403781   1083.711807      3.984364      0.278602       2.639035   \n",
      "min        0.000000      6.000000      0.000000      0.000000       0.000000   \n",
      "25%        0.000000    127.000000      9.000000      0.000000       9.000000   \n",
      "50%        0.000000    905.000000     12.000000      0.000000      11.000000   \n",
      "75%        0.000000   2174.000000     14.000000      0.000000      13.000000   \n",
      "max        1.000000   4116.000000     42.000000     10.000000      15.000000   \n",
      "\n",
      "         screen_num       des_len  avg_tweet_per_day  \n",
      "count  14657.000000  14657.000000       14657.000000  \n",
      "mean       0.330013     60.384663           9.649856  \n",
      "std        1.001933     55.919904          28.241281  \n",
      "min        0.000000      1.000000           0.000000  \n",
      "25%        0.000000      1.000000           0.740741  \n",
      "50%        0.000000     50.000000           1.679957  \n",
      "75%        0.000000    110.000000           8.780503  \n",
      "max       11.000000    436.000000        1212.449275  \n"
     ]
    }
   ],
   "source": [
    "# add average tweet per day\n",
    "usert['avg_tweet_per_day'] = usert['statuses_count']/usert['age']\n",
    "# Now let's look at the new dataframe!\n",
    "print usert.head()\n",
    "print \"\"\n",
    "print usert.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the desired features, we will start training our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classifier: training\n",
    "\n",
    "We will split the whole *usert* dataframe into 80% training and 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11725, 16) (11725L,)\n",
      "(2932, 16) (2932L,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(usert, user.user_type, test_size=0.2, random_state=0)\n",
    "\n",
    "# check the size\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-462bfdd37ea8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;31m# Initial version: use default setting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\logistic.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=np.float64,\n\u001b[0;32m-> 1173\u001b[0;31m                          order=\"C\")\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    519\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    520\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# run the actual classifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initial version: use default setting\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "pred = logreg.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print \"Logistic Regression report:\",\n",
    "print classification_report(y_test, pred)\n",
    "print \"Logistic Regression accuracy:\", accuracy_score(y_test, y_pred_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
