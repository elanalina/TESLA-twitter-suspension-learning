{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Spam Learning: Classifer\n",
    "## CSCE 670 Spring 2018, Course Project\n",
    "### By: Rose Lin (826009602)\n",
    "\n",
    "There will be series of notebooks outlining how we train our models. This one looks into [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), [random forest](https://en.wikipedia.org/wiki/Random_forest) and [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the aggregated user data\n",
    "\n",
    "Our main data sources come from the [Bot Repository](https://botometer.iuni.iu.edu/bot-repository/datasets.html):\n",
    "\n",
    "* Varol-2017, which was released in 2017. It contains 2,573 user IDs crawled in April 2016. We repeatedly called Twitter API to crawl account and tweet information. Despite some suspended accounts, we were able to get information of 2,401 users. \n",
    "* cresci-2017, annotated by CrowdFlower contributors. We downloded the whole dataset and used the following labels: genuine, social_spambots_1, social_spambots_2, social_spambots_3, traditional_spambots_1, traditional_spambots_2, traditional_spambots_3, and traditional_spambots_4.\n",
    "\n",
    "Initially, our aggregated user dataset was imbalanced. We had ~7,000 labeled spammers and ~5,000 labeled legitimate users. To balance it out, we did not use oversampling/downsampling; rather, we utilized Twitter API again. Our crawler started from President Trump's [Twitter account](https://twitter.com/realDonaldTrump) and scraped his friends lists, his friends' following lists, and so on until we collected 2,000 rows of user data. We assume that President Trump is following real users, and his friends follow authentic accounts as well. In this way, we were able to gather a balanced user dataset with spammers:legitimate users ratio roughly to be 1:1. It is acknowledged that our aggregated dataset may subject to biases. If time permits, we will collect a larger dataset that covers as many groups as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the user data...\n",
      "Number of users: 14202\n",
      "Number of spammers: 7293\n",
      "Number of legitmate users: 7364\n"
     ]
    }
   ],
   "source": [
    "#Loading the aggregated data\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print \"Loading the user data...\"\n",
    "user = pd.read_csv(cwd+\"/all_users_balanced.csv\",sep=',',header='infer')\n",
    "print \"Number of users:\",user.id.nunique()\n",
    "print \"Number of spammers:\",len(user[user.user_type == 1])\n",
    "print \"Number of legitmate users:\",len(user[user.user_type == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors_enabled</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>created_at</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>follow_request_sent</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following</th>\n",
       "      <th>...</th>\n",
       "      <th>profile_text_color</th>\n",
       "      <th>profile_use_background_image</th>\n",
       "      <th>protected</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>url</th>\n",
       "      <th>user_type</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2014-04-19 14:46:19</td>\n",
       "      <td>Tue Mar 17 08:51:12 +0000 2009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>22</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>davideb66</td>\n",
       "      <td>1299</td>\n",
       "      <td>Rome</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-18 23:20:58</td>\n",
       "      <td>Sun Apr 19 14:38:04 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Autrice del libro #unavitatuttacurve dal 9 apr...</td>\n",
       "      <td>16358</td>\n",
       "      <td></td>\n",
       "      <td>12561</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>ElisaDospina</td>\n",
       "      <td>18665</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>http://t.co/ceK8TovxwI</td>\n",
       "      <td>1</td>\n",
       "      <td>-7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-13 23:21:54</td>\n",
       "      <td>Wed May 13 15:34:41 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Live Long and Prosper]</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>600</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>Vladimir65</td>\n",
       "      <td>22987</td>\n",
       "      <td>Rome</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-19 23:24:18</td>\n",
       "      <td>Wed Jul 15 12:55:03 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Cuasi Odontologa*♥,#Bipolar, #Sarcastica &amp; Som...</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>398</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3E4415</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>RafielaMorales</td>\n",
       "      <td>7975</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-25200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-11 23:22:23</td>\n",
       "      <td>Wed Aug 05 21:12:49 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>I shall rise from my own death, to avenge hers...</td>\n",
       "      <td>162</td>\n",
       "      <td></td>\n",
       "      <td>413</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>D67345</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>FabrizioC_c</td>\n",
       "      <td>20218</td>\n",
       "      <td>Rome</td>\n",
       "      <td>http://t.co/PK5F0JDKcy</td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors_enabled           crawled_at                      created_at  \\\n",
       "0                       2014-04-19 14:46:19  Tue Mar 17 08:51:12 +0000 2009   \n",
       "1                       2014-05-18 23:20:58  Sun Apr 19 14:38:04 +0000 2009   \n",
       "2                       2014-05-13 23:21:54  Wed May 13 15:34:41 +0000 2009   \n",
       "3                       2014-05-19 23:24:18  Wed Jul 15 12:55:03 +0000 2009   \n",
       "4                       2014-05-11 23:22:23  Wed Aug 05 21:12:49 +0000 2009   \n",
       "\n",
       "  default_profile default_profile_image  \\\n",
       "0             1.0                   1.0   \n",
       "1                                         \n",
       "2                                         \n",
       "3                                         \n",
       "4                                         \n",
       "\n",
       "                                         description  favourites_count  \\\n",
       "0                                                                    1   \n",
       "1  Autrice del libro #unavitatuttacurve dal 9 apr...             16358   \n",
       "2                            [Live Long and Prosper]                14   \n",
       "3  Cuasi Odontologa*♥,#Bipolar, #Sarcastica & Som...                11   \n",
       "4  I shall rise from my own death, to avenge hers...               162   \n",
       "\n",
       "  follow_request_sent  followers_count following   ...     profile_text_color  \\\n",
       "0                                   22             ...                 333333   \n",
       "1                                12561             ...                 333333   \n",
       "2                                  600             ...                 333333   \n",
       "3                                  398             ...                 3E4415   \n",
       "4                                  413             ...                 D67345   \n",
       "\n",
       "  profile_use_background_image  protected     screen_name statuses_count  \\\n",
       "0                          1.0                  davideb66           1299   \n",
       "1                          1.0               ElisaDospina          18665   \n",
       "2                          1.0                 Vladimir65          22987   \n",
       "3                          1.0             RafielaMorales           7975   \n",
       "4                          1.0                FabrizioC_c          20218   \n",
       "\n",
       "                    time_zone                     url user_type utc_offset  \\\n",
       "0                        Rome                                 1     7200.0   \n",
       "1                   Greenland  http://t.co/ceK8TovxwI         1    -7200.0   \n",
       "2                        Rome                                 1     7200.0   \n",
       "3  Pacific Time (US & Canada)                                 1   -25200.0   \n",
       "4                        Rome  http://t.co/PK5F0JDKcy         1     7200.0   \n",
       "\n",
       "  verified  \n",
       "0           \n",
       "1           \n",
       "2           \n",
       "3           \n",
       "4           \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the head\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mainly converted the json response from Twitter API into the dataframe, with two additional features:\n",
    "* crawled_at: the date a record was crawled. It will be used for account age computation.\n",
    "* user_type: 0 = normal users, 1 = spammers. It serves as a binary indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Based on the visualization results, we will consider the following account features:\n",
    "\n",
    "* Count of favorite tweets\n",
    "* Friends to follower ratio\n",
    "* Total status count\n",
    "* Default profile image\n",
    "* Default profile\n",
    "* Account ages\n",
    "* Username, count of characters\n",
    "* Username, count of numbers\n",
    "* Screen_name, count of characters\n",
    "* Screen_name, count of numbers\n",
    "* Length of description \n",
    "* Description text\n",
    "* Average tweet per day\n",
    "\n",
    "These features will be derived from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Create a new dataframe to store the result\n",
    "usert = pd.DataFrame()\n",
    "# add count of favorite tweets\n",
    "usert['favorite_count'] = user['favourites_count']\n",
    "# add friends to follower ratio\n",
    "usert['friends_to_followers'] = user['friends_count'] / user['followers_count']\n",
    "# add total status count\n",
    "usert['statuses_count'] = user['statuses_count']\n",
    "# add default profile image\n",
    "temp_df = pd.get_dummies(user['default_profile_image'])\n",
    "temp_df.columns = ['def_p_img_na','def_p_img_false','def_p_img_true']\n",
    "usert = pd.concat([usert, temp_df], axis=1)\n",
    "# add default profile\n",
    "temp_df = pd.get_dummies(user['default_profile'])\n",
    "temp_df.columns = ['def_p_na','def_p_false','def_p_true']\n",
    "usert = pd.concat([usert, temp_df], axis=1)\n",
    "# add account ages \n",
    "agedf = pd.to_datetime(user['crawled_at'])-pd.to_datetime(user['created_at'])\n",
    "usert['age'] = agedf.dt.days\n",
    "# add username, count of characters and letters\n",
    "for index, item in user['name'].iteritems():\n",
    "    letter = 0\n",
    "    num = 0\n",
    "    for c in item:\n",
    "        if c.isalpha():\n",
    "            letter += 1\n",
    "        elif c.isdigit():\n",
    "            num += 1\n",
    "    usert.loc[index,'name_letter'] = letter\n",
    "    usert.loc[index,'name_num'] = num\n",
    "# add screen name, count of characters and letters\n",
    "for index, item in user['screen_name'].iteritems():\n",
    "    letter = 0\n",
    "    num = 0\n",
    "    for c in item:\n",
    "        if c.isalpha():\n",
    "            letter += 1\n",
    "        elif c.isdigit():\n",
    "            num += 1\n",
    "    usert.loc[index,'screen_letter'] = letter\n",
    "    usert.loc[index,'screen_num'] = num\n",
    "# add len of description\n",
    "usert['des_len'] = pd.Series([len(d) for d in user['description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<14656x21265 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 124455 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add description text (TFIDF)\n",
    "import re, string\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "tfidf_transformer = TfidfVectorizer()\n",
    "des_text = tfidf_transformer.fit_transform(user['description'].tolist())\n",
    "des_text\n",
    "# because the description text is tooooo large, we won't add it to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   favorite_count  friends_to_followers  statuses_count  def_p_img_na  \\\n",
      "0               1              1.818182            1299             0   \n",
      "1           16358              0.274023           18665             1   \n",
      "2              14              1.258333           22987             1   \n",
      "3              11              0.879397            7975             1   \n",
      "4             162              0.980630           20218             1   \n",
      "\n",
      "   def_p_img_false  def_p_img_true  def_p_na  def_p_false  def_p_true   age  \\\n",
      "0                0               1         0            0           1  1859   \n",
      "1                0               0         1            0           0  1855   \n",
      "2                0               0         1            0           0  1826   \n",
      "3                0               0         1            0           0  1769   \n",
      "4                0               0         1            0           0  1740   \n",
      "\n",
      "   name_letter  name_num  screen_letter  screen_num  des_len  \\\n",
      "0         13.0       0.0            7.0         2.0        1   \n",
      "1         12.0       0.0           12.0         0.0      134   \n",
      "2         14.0       0.0            8.0         2.0       23   \n",
      "3         15.0       0.0           14.0         0.0      151   \n",
      "4          4.0       0.0           10.0         0.0       79   \n",
      "\n",
      "   avg_tweet_per_day  \n",
      "0           0.698763  \n",
      "1          10.061995  \n",
      "2          12.588719  \n",
      "3           4.508197  \n",
      "4          11.619540  \n",
      "\n",
      "       favorite_count  friends_to_followers  statuses_count  def_p_img_na  \\\n",
      "count    14657.000000          1.432600e+04    14657.000000  14657.000000   \n",
      "mean      2986.562257                   inf    12743.150304      0.678515   \n",
      "std      11686.509649                   NaN    34701.634675      0.467062   \n",
      "min          0.000000          0.000000e+00        0.000000      0.000000   \n",
      "25%          0.000000          5.604008e-01       65.000000      0.000000   \n",
      "50%         17.000000          1.064804e+00     1536.000000      1.000000   \n",
      "75%       1565.000000          4.549167e+00    10905.000000      1.000000   \n",
      "max     410844.000000                   inf   781664.000000      1.000000   \n",
      "\n",
      "       def_p_img_false  def_p_img_true      def_p_na   def_p_false  \\\n",
      "count     14657.000000    14657.000000  14657.000000  14657.000000   \n",
      "mean          0.310227        0.011257      0.582657      0.212254   \n",
      "std           0.462602        0.105506      0.493137      0.408917   \n",
      "min           0.000000        0.000000      0.000000      0.000000   \n",
      "25%           0.000000        0.000000      0.000000      0.000000   \n",
      "50%           0.000000        0.000000      1.000000      0.000000   \n",
      "75%           1.000000        0.000000      1.000000      0.000000   \n",
      "max           1.000000        1.000000      1.000000      1.000000   \n",
      "\n",
      "         def_p_true           age   name_letter      name_num  screen_letter  \\\n",
      "count  14657.000000  14657.000000  14657.000000  14657.000000   14657.000000   \n",
      "mean       0.205090   1235.965545     10.980214      0.027905      10.961998   \n",
      "std        0.403781   1083.711807      3.984364      0.278602       2.639035   \n",
      "min        0.000000      6.000000      0.000000      0.000000       0.000000   \n",
      "25%        0.000000    127.000000      9.000000      0.000000       9.000000   \n",
      "50%        0.000000    905.000000     12.000000      0.000000      11.000000   \n",
      "75%        0.000000   2174.000000     14.000000      0.000000      13.000000   \n",
      "max        1.000000   4116.000000     42.000000     10.000000      15.000000   \n",
      "\n",
      "         screen_num       des_len  avg_tweet_per_day  \n",
      "count  14657.000000  14657.000000       14657.000000  \n",
      "mean       0.330013     60.384663           9.649856  \n",
      "std        1.001933     55.919904          28.241281  \n",
      "min        0.000000      1.000000           0.000000  \n",
      "25%        0.000000      1.000000           0.740741  \n",
      "50%        0.000000     50.000000           1.679957  \n",
      "75%        0.000000    110.000000           8.780503  \n",
      "max       11.000000    436.000000        1212.449275  \n"
     ]
    }
   ],
   "source": [
    "# add average tweet per day\n",
    "usert['avg_tweet_per_day'] = usert['statuses_count']/usert['age']\n",
    "# Now let's look at the new dataframe!\n",
    "print usert.head()\n",
    "print \"\"\n",
    "print usert.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have all the desired features now, we still need to do some final checks so that our classifiers can process these data without any question. We are mainly concern about 1) duplicates, and 2) missing values.\n",
    "\n",
    "## Analysis on duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10289 True\n",
      "There are 1 duplicated records in total.\n"
     ]
    }
   ],
   "source": [
    "# identify duplicate rows in the original dataframe\n",
    "count = 0\n",
    "for index, row in user.duplicated().iteritems():\n",
    "    if row is True:\n",
    "        print index, row\n",
    "        count += 1\n",
    "print \"There are\",count,\"duplicated records in total.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contributors_enabled                                                                0.0\n",
      "crawled_at                                                          2016-04-30 00:00:00\n",
      "created_at                                               Sat Mar 26 11:56:05 +0000 2011\n",
      "default_profile                                                                     0.0\n",
      "default_profile_image                                                               0.0\n",
      "description                           私は支那人です。日本皇军大好！大東亞共榮萬歲！支那事變日軍被迫進入支那解救僑民和駐軍，也是為...\n",
      "favourites_count                                                                  84243\n",
      "follow_request_sent                                                                    \n",
      "followers_count                                                                    2271\n",
      "following                                                                              \n",
      "friends_count                                                                       640\n",
      "geo_enabled                                                                         0.0\n",
      "id                                                                            272383381\n",
      "is_translator                                                                       0.0\n",
      "lang                                                                                 en\n",
      "listed_count                                                                        118\n",
      "location                                                                               \n",
      "name                                                                               福泽谕吉\n",
      "notifications                                                                          \n",
      "profile_background_color                                                         BADFCD\n",
      "profile_background_image_url          http://abs.twimg.com/images/themes/theme12/bg.gif\n",
      "profile_background_image_url_https    https://abs.twimg.com/images/themes/theme12/bg...\n",
      "profile_background_tile                                                             0.0\n",
      "profile_banner_url                                                                     \n",
      "profile_image_url                     http://pbs.twimg.com/profile_images/3788000000...\n",
      "profile_image_url_https               https://pbs.twimg.com/profile_images/378800000...\n",
      "profile_link_color                                                               FF0000\n",
      "profile_sidebar_border_color                                                     F2E195\n",
      "profile_sidebar_fill_color                                                       FFF7CC\n",
      "profile_text_color                                                               0C3E53\n",
      "profile_use_background_image                                                        1.0\n",
      "protected                                                                           0.0\n",
      "screen_name                                                                    fuzeyuji\n",
      "statuses_count                                                                   149351\n",
      "time_zone                                                                        Hawaii\n",
      "url                                                             https://t.co/zZZb5DioO5\n",
      "user_type                                                                             1\n",
      "utc_offset                                                                       -36000\n",
      "verified                                                                            0.0\n",
      "Name: 10289, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# see the duplicate\n",
    "print user.iloc[10289]\n",
    "# drop it from the user and usert dataframes\n",
    "user.drop([10289],inplace=True)\n",
    "usert.drop([10289],inplace=True)\n",
    "# also update description text here\n",
    "des_text = tfidf_transformer.fit_transform(user['description'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicated records in total.\n"
     ]
    }
   ],
   "source": [
    "# Check again\n",
    "count = 0\n",
    "for index, row in user.duplicated().iteritems():\n",
    "    if row is True:\n",
    "        print index, row\n",
    "        count += 1\n",
    "print \"There are\",count,\"duplicated records in total.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are free from duplicates now! How about missing values?\n",
    "\n",
    "## Analysis on missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At column favorite_count # of NA records: 0\n",
      "At column friends_to_followers # of NA records: 331\n",
      "At column statuses_count # of NA records: 0\n",
      "At column def_p_img_na # of NA records: 0\n",
      "At column def_p_img_false # of NA records: 0\n",
      "At column def_p_img_true # of NA records: 0\n",
      "At column def_p_na # of NA records: 0\n",
      "At column def_p_false # of NA records: 0\n",
      "At column def_p_true # of NA records: 0\n",
      "At column age # of NA records: 0\n",
      "At column name_letter # of NA records: 0\n",
      "At column name_num # of NA records: 0\n",
      "At column screen_letter # of NA records: 0\n",
      "At column screen_num # of NA records: 0\n",
      "At column des_len # of NA records: 0\n",
      "At column avg_tweet_per_day # of NA records: 0\n"
     ]
    }
   ],
   "source": [
    "# Check to see if there is any NA\n",
    "for c in usert.columns.values:\n",
    "    print \"At column\",c,\"# of NA records:\",usert[c].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all other columns are good except for the *friends_to_followers* ratio column that contains some NA. Let's see what these records are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465196345 0 0\n",
      "465306140 0 0\n",
      "465318952 0 0\n",
      "465320112 0 0\n",
      "465325633 0 0\n",
      "465328572 0 0\n",
      "465335657 0 0\n",
      "465338328 0 0\n",
      "465343577 0 0\n",
      "465349136 0 0\n",
      "465360460 0 0\n",
      "465366176 0 0\n",
      "465369079 0 0\n",
      "465369276 0 0\n",
      "465371415 0 0\n",
      "465373317 0 0\n",
      "465373507 0 0\n",
      "465373538 0 0\n",
      "465376231 0 0\n",
      "465376509 0 0\n",
      "465378609 0 0\n",
      "465379325 0 0\n",
      "465387088 0 0\n",
      "465387671 0 0\n",
      "465392182 0 0\n",
      "465393290 0 0\n",
      "465398499 0 0\n",
      "465418896 0 0\n",
      "465427410 0 0\n",
      "465428761 0 0\n",
      "465434130 0 0\n",
      "465434388 0 0\n",
      "465435993 0 0\n",
      "465442625 0 0\n",
      "465445130 0 0\n",
      "466109264 0 0\n",
      "466114317 0 0\n",
      "466116893 0 0\n",
      "466121357 0 0\n",
      "466124818 0 0\n",
      "466125372 0 0\n",
      "466126074 0 0\n",
      "466143639 0 0\n",
      "466152441 0 0\n",
      "466154098 0 0\n",
      "466155797 0 0\n",
      "466163583 0 0\n",
      "466175265 0 0\n",
      "466182757 0 0\n",
      "466183322 0 0\n",
      "466184623 0 0\n",
      "466188621 0 0\n",
      "466189042 0 0\n",
      "466189857 0 0\n",
      "466192045 0 0\n",
      "466192470 0 0\n",
      "466194674 0 0\n",
      "466195745 0 0\n",
      "466200874 0 0\n",
      "466205550 0 0\n",
      "466212113 0 0\n",
      "466217028 0 0\n",
      "466217564 0 0\n",
      "466220361 0 0\n",
      "466225850 0 0\n",
      "466226701 0 0\n",
      "466226882 0 0\n",
      "466227486 0 0\n",
      "466229029 0 0\n",
      "466232314 0 0\n",
      "466234508 0 0\n",
      "466235245 0 0\n",
      "466237339 0 0\n",
      "466238747 0 0\n",
      "466239539 0 0\n",
      "466241419 0 0\n",
      "466246135 0 0\n",
      "466247056 0 0\n",
      "466249274 0 0\n",
      "466256821 0 0\n",
      "466257207 0 0\n",
      "466257969 0 0\n",
      "466259699 0 0\n",
      "466264439 0 0\n",
      "466266821 0 0\n",
      "466278877 0 0\n",
      "466283003 0 0\n",
      "466289763 0 0\n",
      "466291288 0 0\n",
      "466292895 0 0\n",
      "466294714 0 0\n",
      "466295605 0 0\n",
      "466298694 0 0\n",
      "466300701 0 0\n",
      "466300896 0 0\n",
      "466303061 0 0\n",
      "466304146 0 0\n",
      "466305983 0 0\n",
      "466308638 0 0\n",
      "466308751 0 0\n",
      "466309651 0 0\n",
      "466310732 0 0\n",
      "466322512 0 0\n",
      "466324791 0 0\n",
      "466326143 0 0\n",
      "466328816 0 0\n",
      "466329548 0 0\n",
      "466330859 0 0\n",
      "466332009 0 0\n",
      "466334990 0 0\n",
      "466335221 0 0\n",
      "466343368 0 0\n",
      "466352701 0 0\n",
      "466357272 0 0\n",
      "466358135 0 0\n",
      "466359810 0 0\n",
      "466362716 0 0\n",
      "466363084 0 0\n",
      "466370213 0 0\n",
      "466371617 0 0\n",
      "466372053 0 0\n",
      "466375110 0 0\n",
      "466376085 0 0\n",
      "466376780 0 0\n",
      "466379768 0 0\n",
      "466381987 0 0\n",
      "466383527 0 0\n",
      "466384281 0 0\n",
      "466386773 0 0\n",
      "466387081 0 0\n",
      "466388338 0 0\n",
      "466394064 0 0\n",
      "466394269 0 0\n",
      "466395997 0 0\n",
      "466398478 0 0\n",
      "466400012 0 0\n",
      "466400825 0 0\n",
      "466401624 0 0\n",
      "466406088 0 0\n",
      "466409574 0 0\n",
      "466413855 0 0\n",
      "466415308 0 0\n",
      "466426767 0 0\n",
      "466429038 0 0\n",
      "466429651 0 0\n",
      "466430387 0 0\n",
      "466432166 0 0\n",
      "466444765 0 0\n",
      "466462018 0 0\n",
      "466462817 0 0\n",
      "466463170 0 0\n",
      "466463528 0 0\n",
      "466472496 0 0\n",
      "466473505 0 0\n",
      "466475273 0 0\n",
      "466475858 0 0\n",
      "466479806 0 0\n",
      "466480215 0 0\n",
      "467056238 0 0\n",
      "467059649 0 0\n",
      "467060651 0 0\n",
      "467060783 0 0\n",
      "467061366 0 0\n",
      "467065641 0 0\n",
      "467067671 0 0\n",
      "467069697 0 0\n",
      "467077394 0 0\n",
      "467079499 0 0\n",
      "467082134 0 0\n",
      "467084434 0 0\n",
      "467092868 0 0\n",
      "467102366 0 0\n",
      "467106023 0 0\n",
      "467109178 0 0\n",
      "467110763 0 0\n",
      "467117750 0 0\n",
      "467119810 0 0\n",
      "467123168 0 0\n",
      "467123525 0 0\n",
      "467124165 0 0\n",
      "467124670 0 0\n",
      "467126150 0 0\n",
      "467126825 0 0\n",
      "467128482 0 0\n",
      "467131009 0 0\n",
      "467133470 0 0\n",
      "467135461 0 0\n",
      "467135856 0 0\n",
      "467136996 0 0\n",
      "467142582 0 0\n",
      "467143029 0 0\n",
      "467144458 0 0\n",
      "467145195 0 0\n",
      "467147087 0 0\n",
      "467149516 0 0\n",
      "467152720 0 0\n",
      "467153239 0 0\n",
      "467153925 0 0\n",
      "467156950 0 0\n",
      "467157688 0 0\n",
      "467157882 0 0\n",
      "467162901 0 0\n",
      "467164236 0 0\n",
      "467176923 0 0\n",
      "467178690 0 0\n",
      "467185497 0 0\n",
      "467186118 0 0\n",
      "467187664 0 0\n",
      "467189683 0 0\n",
      "467191312 0 0\n",
      "467193516 0 0\n",
      "467194119 0 0\n",
      "467194846 0 0\n",
      "467199549 0 0\n",
      "467200089 0 0\n",
      "467200465 0 0\n",
      "467201459 0 0\n",
      "467203615 0 0\n",
      "467203990 0 0\n",
      "467999325 0 0\n",
      "468004034 0 0\n",
      "468014280 0 0\n",
      "468016342 0 0\n",
      "468022909 0 0\n",
      "468025547 0 0\n",
      "468029740 0 0\n",
      "468033321 0 0\n",
      "468036974 0 0\n",
      "468037917 0 0\n",
      "468048856 0 0\n",
      "468055750 0 0\n",
      "468058184 0 0\n",
      "468059646 0 0\n",
      "468062676 0 0\n",
      "468064243 0 0\n",
      "468068306 0 0\n",
      "468071828 0 0\n",
      "468073544 0 0\n",
      "468080210 0 0\n",
      "468083973 0 0\n",
      "468099552 0 0\n",
      "468101494 0 0\n",
      "468102975 0 0\n",
      "468126635 0 0\n",
      "468129657 0 0\n",
      "468133931 0 0\n",
      "468141676 0 0\n",
      "468143141 0 0\n",
      "468144332 0 0\n",
      "468145003 0 0\n",
      "468148455 0 0\n",
      "468155641 0 0\n",
      "538861240 0 0\n",
      "538865597 0 0\n",
      "538877096 0 0\n",
      "538879598 0 0\n",
      "538883948 0 0\n",
      "538887557 0 0\n",
      "538905719 0 0\n",
      "538907144 0 0\n",
      "538910117 0 0\n",
      "538911422 0 0\n",
      "538916096 0 0\n",
      "538927099 0 0\n",
      "538945998 0 0\n",
      "538952010 0 0\n",
      "538989284 0 0\n",
      "538990279 0 0\n",
      "539038007 0 0\n",
      "539045903 0 0\n",
      "539048096 0 0\n",
      "539052577 0 0\n",
      "539060340 0 0\n",
      "539087929 0 0\n",
      "539091058 0 0\n",
      "539094006 0 0\n",
      "539096910 0 0\n",
      "539103877 0 0\n",
      "539109831 0 0\n",
      "539110886 0 0\n",
      "539130256 0 0\n",
      "539150535 0 0\n",
      "539157095 0 0\n",
      "539186544 0 0\n",
      "539219286 0 0\n",
      "539272485 0 0\n",
      "539276634 0 0\n",
      "542129791 0 0\n",
      "542135879 0 0\n",
      "542150978 0 0\n",
      "542152604 0 0\n",
      "542163896 0 0\n",
      "542215293 0 0\n",
      "542216247 0 0\n",
      "542219577 0 0\n",
      "542236806 0 0\n",
      "545207749 0 0\n",
      "545306050 0 0\n",
      "545308370 0 0\n",
      "545309342 0 0\n",
      "545309765 0 0\n",
      "545708233 0 0\n",
      "545708584 0 0\n",
      "2351877541 0 0\n",
      "2351708202 0 0\n",
      "179562837 0 0\n",
      "179295032 0 0\n",
      "3078705434 0 0\n",
      "15488734 0 0\n",
      "56839754 0 0\n",
      "56840437 0 0\n",
      "56872455 0 0\n",
      "57351732 0 0\n",
      "56872758 0 0\n",
      "57097440 0 0\n",
      "57082889 0 0\n",
      "57097621 0 0\n",
      "57351026 0 0\n",
      "3069252070 0 0\n",
      "56210449 0 0\n",
      "3078501312 0 0\n",
      "55926935 0 0\n",
      "55927230 0 0\n",
      "55927062 0 0\n",
      "55926879 0 0\n",
      "55926996 0 0\n",
      "56250124 0 0\n",
      "56158032 0 0\n",
      "3302525926 0 0\n",
      "704523342 476 142\n",
      "923339203 3846 3668\n"
     ]
    }
   ],
   "source": [
    "# Extract the indexes of row that are nan\n",
    "nan_index = [index for index, row in usert.friends_to_followers.iteritems() if np.isnan(row)]\n",
    "for i in nan_index:\n",
    "    record = user.iloc[i]\n",
    "    print record.id, record.friends_count, record.followers_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like most of the nan comes from dividing 0. Nonetheless, we do have two records that appear to have normal friends and follower counts:\n",
    "\n",
    "| id        | friends_count | followers_count |\n",
    "|-----------|---------------|-----------------|\n",
    "| 704523342 | 476           | 142             |\n",
    "| 923339203 | 3846          | 3668            |\n",
    "\n",
    "To properly handle this issue, we would set the ratio to be **100000** if the followers_count is 0 (not infinity because the classifier can't handle infinities). For the two special cases, we hope that proper divison may help mitigate them (the classical python 2 division problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At column friends_to_followers, # of NA records: 0\n"
     ]
    }
   ],
   "source": [
    "# Fixing the friends_to_followers ratio\n",
    "# This way is slower but hopefully more accurate\n",
    "from __future__ import division\n",
    "\n",
    "for index, row in user.iterrows():\n",
    "    if row['followers_count'] == 0:\n",
    "        usert.loc[index,'friends_to_followers'] = 100000\n",
    "    else:\n",
    "        usert.loc[index,'friends_to_followers'] = row['friends_count'] / row['followers_count']\n",
    "\n",
    "# Check if there is still any NA\n",
    "print \"At column friends_to_followers, # of NA records:\",usert['friends_to_followers'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrific! Now we are free from NAs :) We can proceed to the next step: training classifiers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'usert' (DataFrame)\n",
      "Stored 'user' (DataFrame)\n",
      "Stored 'des_text' (csr_matrix)\n"
     ]
    }
   ],
   "source": [
    "# to store the data\n",
    "%store usert\n",
    "%store user\n",
    "%store des_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to restore data\n",
    "%store -r usert\n",
    "%store -r user\n",
    "%store -r des_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classifier: training\n",
    "\n",
    "We will split the whole dataset into 80% training and 20% testing.\n",
    "\n",
    "Given that we have more observations than features, one may argue that we should normalize our data first. Nonetheless, our trained model will be used in online prediction. We have not came up with a way of normalizing input data under the online setting. Thus, we won't perform any transformation on our data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11724, 16) (11724L,)\n",
      "(2932, 16) (2932L,)\n",
      "(11724, 21265) (2932, 21265)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(usert, user.user_type, test_size=0.2, random_state=0)\n",
    "des_text_train, des_text_test = train_test_split(des_text, test_size=0.2, random_state=0)\n",
    "\n",
    "# check the size\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape\n",
    "print des_text_train.shape, des_text_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New feature set - with TFIDF!\n",
    "X_train_new = np.hstack((X_train,des_text_train.toarray()))\n",
    "X_test_new = np.hstack((X_test,des_text_test.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.80      0.82      1518\n",
      "          1       0.80      0.84      0.81      1414\n",
      "\n",
      "avg / total       0.82      0.82      0.82      2932\n",
      "\n",
      "Logistic Regression accuracy: 0.816848567531\n",
      "ROC score: 0.817477865799\n"
     ]
    }
   ],
   "source": [
    "# run the actual classifier\n",
    "# without descrption text\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initial version: use default setting\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "pred = logreg.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print \"Logistic Regression report:\"\n",
    "print classification_report(y_test, pred)\n",
    "print \"Logistic Regression accuracy:\", accuracy_score(y_test, pred)\n",
    "print \"ROC score:\",roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.78      0.80      1518\n",
      "          1       0.77      0.83      0.80      1414\n",
      "\n",
      "avg / total       0.80      0.80      0.80      2932\n",
      "\n",
      "Logistic Regression accuracy: 0.800477489768\n",
      "ROC score: 0.801376876818\n"
     ]
    }
   ],
   "source": [
    "# with text\n",
    "logreg.fit(X_train_new, y_train)\n",
    "pred = logreg.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print \"Logistic Regression report:\"\n",
    "print classification_report(y_test, pred)\n",
    "print \"Logistic Regression accuracy:\", accuracy_score(y_test, pred)\n",
    "print \"ROC score:\",roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like adding description text TFIDF increases the number of features, thus causing an overfitting issue.\n",
    "\n",
    "Next, we will try Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.96      0.95      1518\n",
      "          1       0.96      0.93      0.95      1414\n",
      "\n",
      "avg / total       0.95      0.95      0.95      2932\n",
      "\n",
      "Random Forest accuracy: 0.948158253752\n",
      "ROC score: 0.947559973389\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print \"Random Forest report:\"\n",
    "print classification_report(y_test, pred)\n",
    "print \"Random Forest accuracy:\", accuracy_score(y_test, pred)\n",
    "print \"ROC score:\",roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.99      0.95      1518\n",
      "          1       0.98      0.89      0.94      1414\n",
      "\n",
      "avg / total       0.94      0.94      0.94      2932\n",
      "\n",
      "Random Forest accuracy: 0.941336971351\n",
      "ROC score: 0.939688378776\n"
     ]
    }
   ],
   "source": [
    "# how about w/ text features?\n",
    "clf.fit(X_train_new, y_train)\n",
    "pred = clf.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print \"Random Forest report:\"\n",
    "print classification_report(y_test, pred)\n",
    "print \"Random Forest accuracy:\", accuracy_score(y_test, pred)\n",
    "print \"ROC score:\",roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, random forest suffers when the description text is added. How about SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      1.00      0.84      1518\n",
      "          1       1.00      0.58      0.74      1414\n",
      "\n",
      "avg / total       0.86      0.80      0.79      2932\n",
      "\n",
      "SVM accuracy: 0.798772169168\n",
      "ROC score: 0.791371994342\n"
     ]
    }
   ],
   "source": [
    "# No model adjustment version\n",
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print \"SVM report:\"\n",
    "print classification_report(y_test, pred)\n",
    "print \"SVM accuracy:\", accuracy_score(y_test, pred)\n",
    "print \"ROC score:\",roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  5.80000000e+02   4.58823529e+00   3.79000000e+03 ...,   0.00000000e+00\n",
      "    1.29000000e+02   1.76525384e+00]\n",
      " [  6.84600000e+03   1.66101695e+00   1.71870000e+04 ...,   2.00000000e+00\n",
      "    1.60000000e+02   6.55491991e+00]\n",
      " [  8.00000000e+01   2.63736264e-01   6.39000000e+02 ...,   0.00000000e+00\n",
      "    1.74000000e+02   9.06382979e-01]\n",
      " ..., \n",
      " [  0.00000000e+00   1.00000000e+05   4.70000000e+01 ...,   0.00000000e+00\n",
      "    3.00000000e+01   5.85305106e-02]\n",
      " [  0.00000000e+00   8.39874411e-01   1.80000000e+02 ...,   0.00000000e+00\n",
      "    1.08000000e+02   7.18562874e-02]\n",
      " [  1.00000000e+00   2.17721519e+00   1.13460000e+04 ...,   0.00000000e+00\n",
      "    3.70000000e+01   2.11679104e+01]]\n",
      "[    1     3     5 ..., 11709 11713 11718]\n",
      "[5626 3729]\n"
     ]
    }
   ],
   "source": [
    "# get support vectors\n",
    "print clf.support_vectors_\n",
    "\n",
    "# get indices of support vectors\n",
    "print clf.support_ \n",
    "\n",
    "# get number of support vectors for each class\n",
    "print clf.n_support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how about w/ text features?\n",
    "clf.fit(X_train_new, y_train)\n",
    "pred = clf.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print \"SVM report:\"\n",
    "print classification_report(y_test, pred)\n",
    "print \"SVM accuracy:\", accuracy_score(y_test, pred)\n",
    "print \"ROC score:\",roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(SVM above timeout)\n",
    "\n",
    "Our initial thought was to use LASSO to scale down the size of total features available (especially with TFIDF included). Nonetheless, it seems that LASSO is a variation of the generalized linear model and thus not applicable for this project (we are doing classification instead of regression). Thus, we won't explore further.\n",
    "\n",
    "Below is a summary of model performance: (average reported for precision, recall and F1-score)\n",
    "\n",
    "** NO TFIDF **\n",
    "\n",
    "| Model               | Accuracy | Precision | Recall | F1-Score | ROC Score |\n",
    "|---------------------|----------|-----------|--------|----------|-----------|\n",
    "| Logistic Regression | 0.8168   | 0.82      | 0.82   | 0.82     | 0.8174    |\n",
    "| Random Forest       | 0.9482   | 0.95      | 0.95   | 0.95     | 0.9475    |\n",
    "| SVM                 | 0.7987   | 0.86      | 0.80   | 0.79     | 0.7913    |\n",
    "\n",
    "** WITH TFIDF **\n",
    "\n",
    "| Model               | Accuracy | Precision | Recall | F1-Score | ROC Score |\n",
    "|---------------------|----------|-----------|--------|----------|-----------|\n",
    "| Logistic Regression | 0.8004   | 0.80      | 0.80   | 0.80     | 0.8014    |\n",
    "| Random Forest       | 0.9413   | 0.94      | 0.94   | 0.94     | 0.9397    |\n",
    "| SVM                 |          |           |        |          |           |\n",
    "\n",
    "\n",
    "It seems that without the description text, our models perform a little bit better.\n",
    "\n",
    "Next, we will attempt to fine tune the model and consider output it for our website.\n",
    "\n",
    "## Model Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
