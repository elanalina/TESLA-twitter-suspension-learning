{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Spam Learning: Classifer\n",
    "## CSCE 670 Spring 2018, Course Project\n",
    "### By: Rose Lin (826009602)\n",
    "\n",
    "There will be series of notebooks outlining how we train our models. This one looks into [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), [random forest](https://en.wikipedia.org/wiki/Random_forest) and [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the aggregated user data\n",
    "\n",
    "Our main data sources come from the [Bot Repository](https://botometer.iuni.iu.edu/bot-repository/datasets.html):\n",
    "\n",
    "* Varol-2017, which was released in 2017. It contains 2,573 user IDs crawled in April 2016. We repeatedly called Twitter API to crawl account and tweet information. Despite some suspended accounts, we were able to get information of 2,401 users. \n",
    "* cresci-2017, annotated by CrowdFlower contributors. We downloded the whole dataset and used the following labels: genuine, social_spambots_1, social_spambots_2, social_spambots_3, traditional_spambots_1, traditional_spambots_2, traditional_spambots_3, and traditional_spambots_4.\n",
    "\n",
    "Initially, our aggregated user dataset was imbalanced. We had ~7,000 labeled spammers and ~5,000 labeled legitimate users. To balance it out, we did not use oversampling/downsampling; rather, we utilized Twitter API again. Our crawler started from President Trump's [Twitter account](https://twitter.com/realDonaldTrump) and scraped his friends lists, his friends' following lists, and so on until we collected 2,000 rows of user data. We assume that President Trump is following real users, and his friends follow authentic accounts as well. \n",
    "\n",
    "After the initial run, we noticed that our account based model did not perform well. That is because most data was old (except for President Trump's 2000+ records). Thus, we acquired more data using the Twitter API again. \n",
    "\n",
    "* We were able to acquire additional 9,573 spam accounts information using the streaming API. We filtered out tweet streams on 4/15 and 4/16/2018. We used the following keywords, with the assumption that whoever sent a tweet containing this keyword was a spam account: ['make money from home','enter to win','Credit Card', 'lonely', 'debt','deals','ad', '100% free','Act now','apply online','Click below','Click here', 'Extra cash','Offer expires', 'order now','Save $','Serious cash','Satisfaction guaranteed', 'Supplies are limited', 'trial','Work from home','you are a winner','your income','Weight loss','why pay more']. Sources of the spam keywords: [455 Spam Trigger Words to Avoid in 2018](https://prospect.io/blog/455-email-spam-trigger-words-avoid-2018/), [“SPAM Tweets” – 5 Buzzwords that Attract Spammers](http://www.adweek.com/digital/spam-tweets-5-buzzwords-that-attract-spammers/)\n",
    "\n",
    "* We also scraped 9,464 ham user data using the same assumption as the one for President Trump's above, but this time our initial seed is from Dr. [Philip Guo](https://twitter.com/pgbovine/), Assistant Professor of Cognitive Science at UC San Diego.\n",
    "\n",
    "In this way, we were able to gather a balanced user dataset with spammers:legitimate users ratio roughly to be 1:1 (15,731 spammers, 16,828 legitimate users). It is acknowledged that our aggregated dataset may subject to biases. If time permits, we will collect a larger dataset that covers as many groups as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the user data...\n",
      "Number of users: 32500\n",
      "Number of spammers: 15731\n",
      "Number of legitmate users: 16828\n"
     ]
    }
   ],
   "source": [
    "#Loading the aggregated data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import date\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(\"Loading the user data...\")\n",
    "user = pd.read_csv(cwd+\"/416.csv\",sep=',',header='infer',low_memory=False)\n",
    "print(\"Number of users:\",user.id.nunique())\n",
    "print(\"Number of spammers:\",len(user[user.user_type == '1']))\n",
    "print(\"Number of legitmate users:\",len(user[user.user_type == '0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>contributors_enabled</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>created_at</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>follow_request_sent</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>...</th>\n",
       "      <th>profile_text_color</th>\n",
       "      <th>profile_use_background_image</th>\n",
       "      <th>protected</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>url</th>\n",
       "      <th>user_type</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>2014-04-19 14:46:19</td>\n",
       "      <td>Tue Mar 17 08:51:12 +0000 2009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>davideb66</td>\n",
       "      <td>1299</td>\n",
       "      <td>Rome</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>2014-05-18 23:20:58</td>\n",
       "      <td>Sun Apr 19 14:38:04 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Autrice del libro #unavitatuttacurve dal 9 apr...</td>\n",
       "      <td>16358</td>\n",
       "      <td></td>\n",
       "      <td>12561</td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>ElisaDospina</td>\n",
       "      <td>18665</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>http://t.co/ceK8TovxwI</td>\n",
       "      <td>1</td>\n",
       "      <td>-7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>2014-05-13 23:21:54</td>\n",
       "      <td>Wed May 13 15:34:41 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Live Long and Prosper]</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>600</td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>Vladimir65</td>\n",
       "      <td>22987</td>\n",
       "      <td>Rome</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>2014-05-19 23:24:18</td>\n",
       "      <td>Wed Jul 15 12:55:03 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Cuasi Odontologa*♥,#Bipolar, #Sarcastica &amp; Som...</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>398</td>\n",
       "      <td>...</td>\n",
       "      <td>3E4415</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>RafielaMorales</td>\n",
       "      <td>7975</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-25200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>2014-05-11 23:22:23</td>\n",
       "      <td>Wed Aug 05 21:12:49 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>I shall rise from my own death, to avenge hers...</td>\n",
       "      <td>162</td>\n",
       "      <td></td>\n",
       "      <td>413</td>\n",
       "      <td>...</td>\n",
       "      <td>D67345</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>FabrizioC_c</td>\n",
       "      <td>20218</td>\n",
       "      <td>Rome</td>\n",
       "      <td>http://t.co/PK5F0JDKcy</td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index contributors_enabled           crawled_at  \\\n",
       "0      0                       2014-04-19 14:46:19   \n",
       "1      1                       2014-05-18 23:20:58   \n",
       "2      2                       2014-05-13 23:21:54   \n",
       "3      3                       2014-05-19 23:24:18   \n",
       "4      4                       2014-05-11 23:22:23   \n",
       "\n",
       "                       created_at default_profile default_profile_image  \\\n",
       "0  Tue Mar 17 08:51:12 +0000 2009             1.0                   1.0   \n",
       "1  Sun Apr 19 14:38:04 +0000 2009                                         \n",
       "2  Wed May 13 15:34:41 +0000 2009                                         \n",
       "3  Wed Jul 15 12:55:03 +0000 2009                                         \n",
       "4  Wed Aug 05 21:12:49 +0000 2009                                         \n",
       "\n",
       "                                         description  favourites_count  \\\n",
       "0                                                                    1   \n",
       "1  Autrice del libro #unavitatuttacurve dal 9 apr...             16358   \n",
       "2                            [Live Long and Prosper]                14   \n",
       "3  Cuasi Odontologa*♥,#Bipolar, #Sarcastica & Som...                11   \n",
       "4  I shall rise from my own death, to avenge hers...               162   \n",
       "\n",
       "  follow_request_sent  followers_count   ...    profile_text_color  \\\n",
       "0                                   22   ...                333333   \n",
       "1                                12561   ...                333333   \n",
       "2                                  600   ...                333333   \n",
       "3                                  398   ...                3E4415   \n",
       "4                                  413   ...                D67345   \n",
       "\n",
       "   profile_use_background_image protected     screen_name statuses_count  \\\n",
       "0                           1.0                 davideb66           1299   \n",
       "1                           1.0              ElisaDospina          18665   \n",
       "2                           1.0                Vladimir65          22987   \n",
       "3                           1.0            RafielaMorales           7975   \n",
       "4                           1.0               FabrizioC_c          20218   \n",
       "\n",
       "                    time_zone                     url user_type utc_offset  \\\n",
       "0                        Rome                                 1     7200.0   \n",
       "1                   Greenland  http://t.co/ceK8TovxwI         1    -7200.0   \n",
       "2                        Rome                                 1     7200.0   \n",
       "3  Pacific Time (US & Canada)                                 1   -25200.0   \n",
       "4                        Rome  http://t.co/PK5F0JDKcy         1     7200.0   \n",
       "\n",
       "  verified  \n",
       "0           \n",
       "1           \n",
       "2           \n",
       "3           \n",
       "4           \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the head\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                  int64\n",
       "contributors_enabled                  object\n",
       "crawled_at                            object\n",
       "created_at                            object\n",
       "default_profile                       object\n",
       "default_profile_image                 object\n",
       "description                           object\n",
       "favourites_count                       int64\n",
       "follow_request_sent                   object\n",
       "followers_count                        int64\n",
       "following                             object\n",
       "friends_count                          int64\n",
       "geo_enabled                           object\n",
       "id                                     int64\n",
       "is_translator                         object\n",
       "lang                                  object\n",
       "listed_count                           int64\n",
       "location                              object\n",
       "name                                  object\n",
       "notifications                         object\n",
       "profile_background_color              object\n",
       "profile_background_image_url          object\n",
       "profile_background_image_url_https    object\n",
       "profile_background_tile               object\n",
       "profile_banner_url                    object\n",
       "profile_image_url                     object\n",
       "profile_image_url_https               object\n",
       "profile_link_color                    object\n",
       "profile_sidebar_border_color          object\n",
       "profile_sidebar_fill_color            object\n",
       "profile_text_color                    object\n",
       "profile_use_background_image          object\n",
       "protected                             object\n",
       "screen_name                           object\n",
       "statuses_count                         int64\n",
       "time_zone                             object\n",
       "url                                   object\n",
       "user_type                             object\n",
       "utc_offset                            object\n",
       "verified                              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to reset type here\n",
    "user.contributors_enabled = user.contributors_enabled.astype('category')\n",
    "user.default_profile = user.default_profile.astype('category')\n",
    "user.default_profile_image = user.default_profile_image.astype('category')\n",
    "user.geo_enabled = user.geo_enabled.astype('category')\n",
    "user.is_translator = user.is_translator.astype('category')\n",
    "user.profile_background_tile = user.profile_background_tile.astype('category')\n",
    "user.profile_use_background_image = user.profile_use_background_image.astype('category')\n",
    "user.user_type = pd.to_numeric(user.user_type, downcast='integer', errors='coerce')\n",
    "user.user_type = user.user_type.fillna(0.0).astype('int64')\n",
    "#user.crawled_at = user.crawled_at.astype('datetime64')\n",
    "#user.created_at = user.created_at.astype('datetime64')\n",
    "#user.favourites_count = user.favourites_count.astype('float64')\n",
    "#user.followers_count = user.followers_count.astype('float64')\n",
    "#user.friends_count = user.friends_count.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                    int64\n",
       "contributors_enabled                  category\n",
       "crawled_at                              object\n",
       "created_at                              object\n",
       "default_profile                       category\n",
       "default_profile_image                 category\n",
       "description                             object\n",
       "favourites_count                         int64\n",
       "follow_request_sent                     object\n",
       "followers_count                          int64\n",
       "following                               object\n",
       "friends_count                            int64\n",
       "geo_enabled                           category\n",
       "id                                       int64\n",
       "is_translator                         category\n",
       "lang                                    object\n",
       "listed_count                             int64\n",
       "location                                object\n",
       "name                                    object\n",
       "notifications                           object\n",
       "profile_background_color                object\n",
       "profile_background_image_url            object\n",
       "profile_background_image_url_https      object\n",
       "profile_background_tile               category\n",
       "profile_banner_url                      object\n",
       "profile_image_url                       object\n",
       "profile_image_url_https                 object\n",
       "profile_link_color                      object\n",
       "profile_sidebar_border_color            object\n",
       "profile_sidebar_fill_color              object\n",
       "profile_text_color                      object\n",
       "profile_use_background_image          category\n",
       "protected                               object\n",
       "screen_name                             object\n",
       "statuses_count                           int64\n",
       "time_zone                               object\n",
       "url                                     object\n",
       "user_type                                int64\n",
       "utc_offset                              object\n",
       "verified                                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mainly converted the json response from Twitter API into the dataframe, with two additional features:\n",
    "* crawled_at: the date a record was crawled. It will be used for account age computation.\n",
    "* user_type: 0 = normal users, 1 = spammers. It serves as a binary indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Based on the visualization results, we will consider the following account features:\n",
    "\n",
    "* Count of favorite tweets\n",
    "* Friends to follower ratio\n",
    "* Total status count\n",
    "* Default profile image\n",
    "* Default profile\n",
    "* Account ages\n",
    "* Username, count of characters\n",
    "* Username, count of numbers\n",
    "* Screen_name, count of characters\n",
    "* Screen_name, count of numbers\n",
    "* Length of description \n",
    "* Description text\n",
    "* ~~Average tweet per day~~ (eventually removed because it correlated with total status count and ages)\n",
    "\n",
    "These features will be derived from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Create a new dataframe to store the result\n",
    "usert = pd.DataFrame()\n",
    "# add count of favorite tweets\n",
    "usert['favorite_count'] = user['favourites_count']\n",
    "# add friends to follower ratio\n",
    "usert['friends_to_followers'] = user['friends_count'] / user['followers_count']\n",
    "# add total status count\n",
    "usert['statuses_count'] = user['statuses_count']\n",
    "# add default profile image\n",
    "temp_df = pd.get_dummies(user['default_profile_image'])\n",
    "temp_df.columns = ['def_p_img_na','def_p_img_false','def_p_img_true']\n",
    "usert = pd.concat([usert, temp_df], axis=1)\n",
    "# add default profile\n",
    "temp_df = pd.get_dummies(user['default_profile'])\n",
    "temp_df.columns = ['def_p_na','def_p_false','def_p_true']\n",
    "usert = pd.concat([usert, temp_df], axis=1)\n",
    "# add account ages \n",
    "agedf = pd.to_datetime(user['crawled_at'])-pd.to_datetime(user['created_at'])\n",
    "usert['age'] = agedf.dt.days\n",
    "# add username, count of characters and letters\n",
    "for index, item in user['name'].items():\n",
    "    letter = 0\n",
    "    num = 0\n",
    "    for c in item:\n",
    "        if c.isalpha():\n",
    "            letter += 1\n",
    "        elif c.isdigit():\n",
    "            num += 1\n",
    "    usert.loc[index,'name_letter'] = letter\n",
    "    usert.loc[index,'name_num'] = num\n",
    "# add screen name, count of characters and letters\n",
    "for index, item in user['screen_name'].items():\n",
    "    letter = 0\n",
    "    num = 0\n",
    "    for c in item:\n",
    "        if c.isalpha():\n",
    "            letter += 1\n",
    "        elif c.isdigit():\n",
    "            num += 1\n",
    "    usert.loc[index,'screen_letter'] = letter\n",
    "    usert.loc[index,'screen_num'] = num\n",
    "# add len of description\n",
    "usert['des_len'] = pd.Series([len(d) for d in user['description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no description text (TFIDF)\n",
    "#import re, string\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#tfidf_transformer = TfidfVectorizer()\n",
    "#des_text = tfidf_transformer.fit_transform(user['description'].tolist())\n",
    "#des_text\n",
    "# because the description text is tooooo large, we won't add it to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   favorite_count  friends_to_followers  statuses_count  def_p_img_na  \\\n",
      "0               1              1.818182            1299             0   \n",
      "1           16358              0.274023           18665             1   \n",
      "2              14              1.258333           22987             1   \n",
      "3              11              0.879397            7975             1   \n",
      "4             162              0.980630           20218             1   \n",
      "\n",
      "   def_p_img_false  def_p_img_true  def_p_na  def_p_false  def_p_true   age  \\\n",
      "0                0               1         0            0           1  1859   \n",
      "1                0               0         1            0           0  1855   \n",
      "2                0               0         1            0           0  1826   \n",
      "3                0               0         1            0           0  1769   \n",
      "4                0               0         1            0           0  1740   \n",
      "\n",
      "   name_letter  name_num  screen_letter  screen_num  des_len  \n",
      "0         13.0       0.0            7.0         2.0        1  \n",
      "1         12.0       0.0           12.0         0.0      134  \n",
      "2         14.0       0.0            8.0         2.0       23  \n",
      "3         15.0       0.0           14.0         0.0      149  \n",
      "4          4.0       0.0           10.0         0.0       79  \n",
      "\n",
      "       favorite_count  friends_to_followers  statuses_count  def_p_img_na  \\\n",
      "count    33687.000000          3.330600e+04    3.368700e+04  33687.000000   \n",
      "mean      6994.511355                   inf    2.064410e+04      0.295218   \n",
      "std      22399.560307                   NaN    7.720248e+04      0.456147   \n",
      "min          0.000000          0.000000e+00    0.000000e+00      0.000000   \n",
      "25%          3.000000          2.834203e-01    3.600000e+02      0.000000   \n",
      "50%        531.000000          9.193548e-01    3.531000e+03      0.000000   \n",
      "75%       4345.000000          2.162493e+00    1.547800e+04      1.000000   \n",
      "max     657248.000000                   inf    8.250670e+06      1.000000   \n",
      "\n",
      "       def_p_img_false  def_p_img_true      def_p_na   def_p_false  \\\n",
      "count     33687.000000    33687.000000  33687.000000  33687.000000   \n",
      "mean          0.697569        0.007213      0.253510      0.418945   \n",
      "std           0.459318        0.084626      0.435027      0.493394   \n",
      "min           0.000000        0.000000      0.000000      0.000000   \n",
      "25%           0.000000        0.000000      0.000000      0.000000   \n",
      "50%           1.000000        0.000000      0.000000      0.000000   \n",
      "75%           1.000000        0.000000      1.000000      1.000000   \n",
      "max           1.000000        1.000000      1.000000      1.000000   \n",
      "\n",
      "         def_p_true           age   name_letter      name_num  screen_letter  \\\n",
      "count  33687.000000  33687.000000  33687.000000  33687.000000   33687.000000   \n",
      "mean       0.327545   1714.254312     11.101642      0.053552      10.300591   \n",
      "std        0.469325   1235.333933      4.274657      0.421453       2.950854   \n",
      "min        0.000000     -2.000000      0.000000      0.000000       0.000000   \n",
      "25%        0.000000    511.000000      9.000000      0.000000       8.000000   \n",
      "50%        0.000000   1697.000000     11.000000      0.000000      11.000000   \n",
      "75%        1.000000   2788.000000     14.000000      0.000000      13.000000   \n",
      "max        1.000000   4408.000000     45.000000     14.000000      15.000000   \n",
      "\n",
      "         screen_num       des_len  \n",
      "count  33687.000000  33687.000000  \n",
      "mean       0.450411     74.265830  \n",
      "std        1.233415     57.084016  \n",
      "min        0.000000      1.000000  \n",
      "25%        0.000000     15.000000  \n",
      "50%        0.000000     73.000000  \n",
      "75%        0.000000    128.000000  \n",
      "max       14.000000    343.000000  \n"
     ]
    }
   ],
   "source": [
    "# add average tweet per day\n",
    "#usert['avg_tweet_per_day'] = usert['statuses_count']/usert['age']\n",
    "# Now let's look at the new dataframe!\n",
    "print(usert.head())\n",
    "print(\"\")\n",
    "print(usert.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have all the desired features now, we still need to do some final checks so that our classifiers can process these data without any question. We are mainly concern about 1) duplicates, and 2) missing values.\n",
    "\n",
    "## Analysis on duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicated records in total.\n"
     ]
    }
   ],
   "source": [
    "# identify duplicate rows in the original dataframe\n",
    "count = 0\n",
    "for index, row in user.duplicated().items():\n",
    "    if row is True:\n",
    "        print(index, row)\n",
    "        count += 1\n",
    "print(\"There are\",count,\"duplicated records in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are free from duplicates now! How about missing values?\n",
    "\n",
    "## Analysis on missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At column favorite_count # of NA records: 0\n",
      "At column friends_to_followers # of NA records: 381\n",
      "At column statuses_count # of NA records: 0\n",
      "At column def_p_img_na # of NA records: 0\n",
      "At column def_p_img_false # of NA records: 0\n",
      "At column def_p_img_true # of NA records: 0\n",
      "At column def_p_na # of NA records: 0\n",
      "At column def_p_false # of NA records: 0\n",
      "At column def_p_true # of NA records: 0\n",
      "At column age # of NA records: 0\n",
      "At column name_letter # of NA records: 0\n",
      "At column name_num # of NA records: 0\n",
      "At column screen_letter # of NA records: 0\n",
      "At column screen_num # of NA records: 0\n",
      "At column des_len # of NA records: 0\n"
     ]
    }
   ],
   "source": [
    "# Check to see if there is any NA\n",
    "for c in usert.columns.values:\n",
    "    print(\"At column\",c,\"# of NA records:\",usert[c].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all other columns are good except for the *friends_to_followers* ratio column that contains some NA. Let's see what these records are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465196345 0 0\n",
      "465306140 0 0\n",
      "465318952 0 0\n",
      "465320112 0 0\n",
      "465325633 0 0\n",
      "465328572 0 0\n",
      "465335657 0 0\n",
      "465338328 0 0\n",
      "465343577 0 0\n",
      "465349136 0 0\n",
      "465360460 0 0\n",
      "465366176 0 0\n",
      "465369079 0 0\n",
      "465369276 0 0\n",
      "465371415 0 0\n",
      "465373317 0 0\n",
      "465373507 0 0\n",
      "465373538 0 0\n",
      "465376231 0 0\n",
      "465376509 0 0\n",
      "465378609 0 0\n",
      "465379325 0 0\n",
      "465387088 0 0\n",
      "465387671 0 0\n",
      "465392182 0 0\n",
      "465393290 0 0\n",
      "465398499 0 0\n",
      "465418896 0 0\n",
      "465427410 0 0\n",
      "465428761 0 0\n",
      "465434130 0 0\n",
      "465434388 0 0\n",
      "465435993 0 0\n",
      "465442625 0 0\n",
      "465445130 0 0\n",
      "466109264 0 0\n",
      "466114317 0 0\n",
      "466116893 0 0\n",
      "466121357 0 0\n",
      "466124818 0 0\n",
      "466125372 0 0\n",
      "466126074 0 0\n",
      "466143639 0 0\n",
      "466152441 0 0\n",
      "466154098 0 0\n",
      "466155797 0 0\n",
      "466163583 0 0\n",
      "466175265 0 0\n",
      "466182757 0 0\n",
      "466183322 0 0\n",
      "466184623 0 0\n",
      "466188621 0 0\n",
      "466189042 0 0\n",
      "466189857 0 0\n",
      "466192045 0 0\n",
      "466192470 0 0\n",
      "466194674 0 0\n",
      "466195745 0 0\n",
      "466200874 0 0\n",
      "466205550 0 0\n",
      "466212113 0 0\n",
      "466217028 0 0\n",
      "466217564 0 0\n",
      "466220361 0 0\n",
      "466225850 0 0\n",
      "466226701 0 0\n",
      "466226882 0 0\n",
      "466227486 0 0\n",
      "466229029 0 0\n",
      "466232314 0 0\n",
      "466234508 0 0\n",
      "466235245 0 0\n",
      "466237339 0 0\n",
      "466238747 0 0\n",
      "466239539 0 0\n",
      "466241419 0 0\n",
      "466246135 0 0\n",
      "466247056 0 0\n",
      "466249274 0 0\n",
      "466256821 0 0\n",
      "466257207 0 0\n",
      "466257969 0 0\n",
      "466259699 0 0\n",
      "466264439 0 0\n",
      "466266821 0 0\n",
      "466278877 0 0\n",
      "466283003 0 0\n",
      "466289763 0 0\n",
      "466291288 0 0\n",
      "466292895 0 0\n",
      "466294714 0 0\n",
      "466295605 0 0\n",
      "466298694 0 0\n",
      "466300701 0 0\n",
      "466300896 0 0\n",
      "466303061 0 0\n",
      "466304146 0 0\n",
      "466305983 0 0\n",
      "466308638 0 0\n",
      "466308751 0 0\n",
      "466309651 0 0\n",
      "466310732 0 0\n",
      "466322512 0 0\n",
      "466324791 0 0\n",
      "466326143 0 0\n",
      "466328816 0 0\n",
      "466329548 0 0\n",
      "466330859 0 0\n",
      "466332009 0 0\n",
      "466334990 0 0\n",
      "466335221 0 0\n",
      "466343368 0 0\n",
      "466352701 0 0\n",
      "466357272 0 0\n",
      "466358135 0 0\n",
      "466359810 0 0\n",
      "466362716 0 0\n",
      "466363084 0 0\n",
      "466370213 0 0\n",
      "466371617 0 0\n",
      "466372053 0 0\n",
      "466375110 0 0\n",
      "466376085 0 0\n",
      "466376780 0 0\n",
      "466379768 0 0\n",
      "466381987 0 0\n",
      "466383527 0 0\n",
      "466384281 0 0\n",
      "466386773 0 0\n",
      "466387081 0 0\n",
      "466388338 0 0\n",
      "466394064 0 0\n",
      "466394269 0 0\n",
      "466395997 0 0\n",
      "466398478 0 0\n",
      "466400012 0 0\n",
      "466400825 0 0\n",
      "466401624 0 0\n",
      "466406088 0 0\n",
      "466409574 0 0\n",
      "466413855 0 0\n",
      "466415308 0 0\n",
      "466426767 0 0\n",
      "466429038 0 0\n",
      "466429651 0 0\n",
      "466430387 0 0\n",
      "466432166 0 0\n",
      "466444765 0 0\n",
      "466462018 0 0\n",
      "466462817 0 0\n",
      "466463170 0 0\n",
      "466463528 0 0\n",
      "466472496 0 0\n",
      "466473505 0 0\n",
      "466475273 0 0\n",
      "466475858 0 0\n",
      "466479806 0 0\n",
      "466480215 0 0\n",
      "467056238 0 0\n",
      "467059649 0 0\n",
      "467060651 0 0\n",
      "467060783 0 0\n",
      "467061366 0 0\n",
      "467065641 0 0\n",
      "467067671 0 0\n",
      "467069697 0 0\n",
      "467077394 0 0\n",
      "467079499 0 0\n",
      "467082134 0 0\n",
      "467084434 0 0\n",
      "467092868 0 0\n",
      "467102366 0 0\n",
      "467106023 0 0\n",
      "467109178 0 0\n",
      "467110763 0 0\n",
      "467117750 0 0\n",
      "467119810 0 0\n",
      "467123168 0 0\n",
      "467123525 0 0\n",
      "467124165 0 0\n",
      "467124670 0 0\n",
      "467126150 0 0\n",
      "467126825 0 0\n",
      "467128482 0 0\n",
      "467131009 0 0\n",
      "467133470 0 0\n",
      "467135461 0 0\n",
      "467135856 0 0\n",
      "467136996 0 0\n",
      "467142582 0 0\n",
      "467143029 0 0\n",
      "467144458 0 0\n",
      "467145195 0 0\n",
      "467147087 0 0\n",
      "467149516 0 0\n",
      "467152720 0 0\n",
      "467153239 0 0\n",
      "467153925 0 0\n",
      "467156950 0 0\n",
      "467157688 0 0\n",
      "467157882 0 0\n",
      "467162901 0 0\n",
      "467164236 0 0\n",
      "467176923 0 0\n",
      "467178690 0 0\n",
      "467185497 0 0\n",
      "467186118 0 0\n",
      "467187664 0 0\n",
      "467189683 0 0\n",
      "467191312 0 0\n",
      "467193516 0 0\n",
      "467194119 0 0\n",
      "467194846 0 0\n",
      "467199549 0 0\n",
      "467200089 0 0\n",
      "467200465 0 0\n",
      "467201459 0 0\n",
      "467203615 0 0\n",
      "467203990 0 0\n",
      "467999325 0 0\n",
      "468004034 0 0\n",
      "468014280 0 0\n",
      "468016342 0 0\n",
      "468022909 0 0\n",
      "468025547 0 0\n",
      "468029740 0 0\n",
      "468033321 0 0\n",
      "468036974 0 0\n",
      "468037917 0 0\n",
      "468048856 0 0\n",
      "468055750 0 0\n",
      "468058184 0 0\n",
      "468059646 0 0\n",
      "468062676 0 0\n",
      "468064243 0 0\n",
      "468068306 0 0\n",
      "468071828 0 0\n",
      "468073544 0 0\n",
      "468080210 0 0\n",
      "468083973 0 0\n",
      "468099552 0 0\n",
      "468101494 0 0\n",
      "468102975 0 0\n",
      "468126635 0 0\n",
      "468129657 0 0\n",
      "468133931 0 0\n",
      "468141676 0 0\n",
      "468143141 0 0\n",
      "468144332 0 0\n",
      "468145003 0 0\n",
      "468148455 0 0\n",
      "468155641 0 0\n",
      "538861240 0 0\n",
      "538865597 0 0\n",
      "538877096 0 0\n",
      "538879598 0 0\n",
      "538883948 0 0\n",
      "538887557 0 0\n",
      "538905719 0 0\n",
      "538907144 0 0\n",
      "538910117 0 0\n",
      "538911422 0 0\n",
      "538916096 0 0\n",
      "538927099 0 0\n",
      "538945998 0 0\n",
      "538952010 0 0\n",
      "538989284 0 0\n",
      "538990279 0 0\n",
      "539038007 0 0\n",
      "539045903 0 0\n",
      "539048096 0 0\n",
      "539052577 0 0\n",
      "539060340 0 0\n",
      "539087929 0 0\n",
      "539091058 0 0\n",
      "539094006 0 0\n",
      "539096910 0 0\n",
      "539103877 0 0\n",
      "539109831 0 0\n",
      "539110886 0 0\n",
      "539130256 0 0\n",
      "539150535 0 0\n",
      "539157095 0 0\n",
      "539186544 0 0\n",
      "539219286 0 0\n",
      "539272485 0 0\n",
      "539276634 0 0\n",
      "542129791 0 0\n",
      "542135879 0 0\n",
      "542150978 0 0\n",
      "542152604 0 0\n",
      "542163896 0 0\n",
      "542215293 0 0\n",
      "542216247 0 0\n",
      "542219577 0 0\n",
      "542236806 0 0\n",
      "545207749 0 0\n",
      "545306050 0 0\n",
      "545308370 0 0\n",
      "545309342 0 0\n",
      "545309765 0 0\n",
      "545708233 0 0\n",
      "545708584 0 0\n",
      "2351877541 0 0\n",
      "2351708202 0 0\n",
      "179562837 0 0\n",
      "179295032 0 0\n",
      "3078705434 0 0\n",
      "15488734 0 0\n",
      "56839754 0 0\n",
      "56840437 0 0\n",
      "56872455 0 0\n",
      "57351732 0 0\n",
      "56872758 0 0\n",
      "57097440 0 0\n",
      "57082889 0 0\n",
      "57097621 0 0\n",
      "57351026 0 0\n",
      "3069252070 0 0\n",
      "56210449 0 0\n",
      "3078501312 0 0\n",
      "55926935 0 0\n",
      "55927230 0 0\n",
      "55927062 0 0\n",
      "55926879 0 0\n",
      "55926996 0 0\n",
      "56250124 0 0\n",
      "56158032 0 0\n",
      "3302525926 0 0\n",
      "3287882455 0 0\n",
      "3400714546 0 0\n",
      "950399726792327170 0 0\n",
      "984984544645443585 0 0\n",
      "985728515122151424 0 0\n",
      "983893320157073408 0 0\n",
      "984818074279964678 0 0\n",
      "984396963721265157 0 0\n",
      "863696332363575296 0 0\n",
      "985856523988602881 0 0\n",
      "981722331302199296 0 0\n",
      "971920526934331394 0 0\n",
      "971921697661046784 0 0\n",
      "971572913315856385 0 0\n",
      "971923860474937344 0 0\n",
      "971924616942821379 0 0\n",
      "979629628670316544 0 0\n",
      "971926603902013440 0 0\n",
      "971928303450783744 0 0\n",
      "971932098377338881 0 0\n",
      "971933163004903424 0 0\n",
      "984549474180661248 0 0\n",
      "984765488109707271 0 0\n",
      "971934546164826113 0 0\n",
      "971935724512600064 0 0\n",
      "971201083186995200 0 0\n",
      "971937104467935232 0 0\n",
      "971938434531995648 0 0\n",
      "971940262380371969 0 0\n",
      "971940933112483842 0 0\n",
      "971942460539547650 0 0\n",
      "971944707088465921 0 0\n",
      "971945784169005056 0 0\n",
      "979630348354146304 0 0\n",
      "443515349 0 0\n",
      "979981997412990978 0 0\n",
      "725420430 0 0\n",
      "984446293853356033 0 0\n",
      "983351837565874176 0 0\n",
      "984226870341357568 0 0\n",
      "985863466681622528 0 0\n",
      "981128714783199233 0 0\n",
      "411166063 0 0\n",
      "959237277506453504 0 0\n",
      "985870738732154881 0 0\n",
      "969519355561107456 0 0\n",
      "918806040329666561 0 0\n",
      "985856927933476864 0 0\n",
      "565229453 0 0\n",
      "979632842861891584 0 0\n",
      "3439063295 0 0\n",
      "984984544645443585 0 0\n"
     ]
    }
   ],
   "source": [
    "# Extract the indexes of row that are nan\n",
    "nan_index = [index for index, row in usert.friends_to_followers.items() if np.isnan(row)]\n",
    "for i in nan_index:\n",
    "    record = user.iloc[i]\n",
    "    print(record.id, record.friends_count, record.followers_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like most of the nan comes from dividing 0.\n",
    "\n",
    "To properly handle this issue, we would set the ratio to be **100000** if the followers_count is 0 (not infinity because the classifier can't handle infinities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At column friends_to_followers, # of NA records: 0\n"
     ]
    }
   ],
   "source": [
    "# Fixing the friends_to_followers ratio\n",
    "# This way is slower but hopefully more accurate\n",
    "for index, row in user.iterrows():\n",
    "    if row['followers_count'] == 0:\n",
    "        usert.loc[index,'friends_to_followers'] = 100000\n",
    "\n",
    "# Check if there is still any NA\n",
    "print(\"At column friends_to_followers, # of NA records:\",usert['friends_to_followers'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrific! Now we are free from NAs :) We can proceed to the next step: training classifiers!\n",
    "\n",
    "Note: we remove *avg_tweet_per_day* here and save the remaining features into another dataframe as it correlates with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'usert' (DataFrame)\n",
      "Stored 'user' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# to store the data\n",
    "%store usert\n",
    "%store user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to restore data\n",
    "%store -r usert\n",
    "%store -r user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classifier: training\n",
    "\n",
    "We will split the whole dataset into 80% training and 20% testing.\n",
    "\n",
    "Given that we have more observations than features, one may argue that we should normalize our data first. Nonetheless, our trained model will be used in online prediction. We have not came up with a way of normalizing input data under the online setting. Thus, we won't perform any transformation on our data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26949, 15) (26949,)\n",
      "(6738, 15) (6738,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(usert, user.user_type, test_size=0.2, random_state=0)\n",
    "#des_text_train, des_text_test = train_test_split(des_text, test_size=0.2, random_state=0)\n",
    "\n",
    "# check the size\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "#print(des_text_train.shape, des_text_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New feature set - with TFIDF!\n",
    "#X_train_new = np.hstack((X_train,des_text_train.toarray()))\n",
    "#X_test_new = np.hstack((X_test,des_text_test.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.77      0.75      3419\n",
      "          1       0.75      0.72      0.73      3319\n",
      "\n",
      "avg / total       0.75      0.75      0.74      6738\n",
      "\n",
      "Logistic Regression accuracy: 0.7451766102701098\n",
      "ROC score: 0.7447525529710484\n"
     ]
    }
   ],
   "source": [
    "# run the actual classifier\n",
    "# without descrption text\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initial version: use default setting\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "pred = logreg.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"Logistic Regression report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Logistic Regression accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with text\n",
    "#logreg.fit(X_train_new, y_train)\n",
    "#pred = logreg.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "#print(\"Logistic Regression report:\")\n",
    "#print(classification_report(y_test, pred))\n",
    "#print(\"Logistic Regression accuracy:\", accuracy_score(y_test, pred))\n",
    "#print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like adding description text TFIDF increases the number of features, thus causing an overfitting issue.\n",
    "\n",
    "Next, we will try Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.90      0.88      3419\n",
      "          1       0.89      0.84      0.87      3319\n",
      "\n",
      "avg / total       0.87      0.87      0.87      6738\n",
      "\n",
      "Random Forest accuracy: 0.8725140991392104\n",
      "ROC score: 0.8720828459715179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"Random Forest report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Random Forest accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how about w/ text features?\n",
    "#rf.fit(X_train_new, y_train)\n",
    "#pred = rf.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "#print(\"Random Forest report:\")\n",
    "#print(classification_report(y_test, pred))\n",
    "#print(\"Random Forest accuracy:\", accuracy_score(y_test, pred))\n",
    "#print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, random forest suffers when the description text is added. How about SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      1.00      0.74      3419\n",
      "          1       1.00      0.29      0.45      3319\n",
      "\n",
      "avg / total       0.79      0.65      0.60      6738\n",
      "\n",
      "SVM accuracy: 0.651231819531018\n",
      "ROC score: 0.6459777041277494\n"
     ]
    }
   ],
   "source": [
    "# No model adjustment version\n",
    "from sklearn import svm\n",
    "\n",
    "svm = svm.SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "pred = svm.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"SVM report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"SVM accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.04000000e+02 2.91666667e+00 1.96000000e+02 ... 1.00000000e+01\n",
      "  0.00000000e+00 7.70000000e+01]\n",
      " [9.00000000e+01 5.82877960e-02 7.98000000e+02 ... 1.30000000e+01\n",
      "  0.00000000e+00 1.43000000e+02]\n",
      " [1.05000000e+02 9.76627713e-01 4.05900000e+03 ... 9.00000000e+00\n",
      "  0.00000000e+00 1.56000000e+02]\n",
      " ...\n",
      " [4.68900000e+03 2.96624088e+00 1.48010000e+04 ... 1.10000000e+01\n",
      "  0.00000000e+00 6.40000000e+01]\n",
      " [1.34780000e+04 7.14845839e-01 1.18595000e+05 ... 1.00000000e+01\n",
      "  0.00000000e+00 1.00000000e+00]\n",
      " [1.08000000e+02 2.55000000e+00 1.42000000e+03 ... 5.00000000e+00\n",
      "  2.00000000e+00 1.00000000e+00]]\n",
      "[    1     2     3 ... 26943 26944 26947]\n",
      "[13022 11366]\n"
     ]
    }
   ],
   "source": [
    "# get support vectors\n",
    "print(svm.support_vectors_)\n",
    "\n",
    "# get indices of support vectors\n",
    "print(svm.support_) \n",
    "\n",
    "# get number of support vectors for each class\n",
    "print(svm.n_support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how about w/ text features?\n",
    "#svm.fit(X_train_new, y_train)\n",
    "#pred = svm.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "#print(\"SVM report:\")\n",
    "#print(classification_report(y_test, pred))\n",
    "#print(\"SVM accuracy:\", accuracy_score(y_test, pred))\n",
    "#print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our initial thought was to use LASSO to scale down the size of total features available (especially with TFIDF included). Nonetheless, it seems that LASSO is a variation of the generalized linear model and thus not applicable for this project (we are doing classification instead of regression). Thus, we won't explore further.\n",
    "\n",
    "Below is a summary of model performance: (average reported for precision, recall and F1-score)\n",
    "\n",
    "** NO DESCRIPTION TFIDF **\n",
    "\n",
    "| Model               | Accuracy | Precision | Recall | F1-Score | ROC Score |\n",
    "|---------------------|----------|-----------|--------|----------|-----------|\n",
    "| Logistic Regression | 0.7451   | 0.75      | 0.75   | 0.74     | 0.7447    |\n",
    "| Random Forest       | 0.8725   | 0.87      | 0.87   | 0.87     | 0.8720    |\n",
    "| SVM                 | 0.6512   | 0.79      | 0.65   | 0.60     | 0.6459    |\n",
    "\n",
    "Next, we will attempt to combine the models together and consider output it for our website.\n",
    "\n",
    "## Model Fine-tuning\n",
    "\n",
    "Based on our observations, Random Forest and KNN both have their accuracies above 93%. We would consider combine them together through the ensemble method to see if we could further push the performance up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble report (RF+KNN):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.96      0.84      3419\n",
      "          1       0.94      0.68      0.79      3319\n",
      "\n",
      "avg / total       0.85      0.82      0.82      6738\n",
      "\n",
      "Ensemble accuracy: 0.8202730780647076\n",
      "ROC score: 0.8181515556377653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\envs\\py3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# source: https://stats.stackexchange.com/questions/139042/ensemble-of-different-kinds-of-regressors-using-scikit-learn-or-any-other-pytho\n",
    "# import knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = RandomForestClassifier()\n",
    "estimators.append(('RF', model1))\n",
    "model2 = KNeighborsClassifier(n_neighbors=5)\n",
    "estimators.append(('KNN', model2))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators,n_jobs=2)\n",
    "ensemble.fit(X_train, y_train)\n",
    "pred = ensemble.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"Ensemble report (RF+KNN):\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Ensemble accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a tradeoff here. Both the accuracy and the ROC score have decreased to around 80%, compared with the original random forest. But precision for spammers and recall for legitimate users increase correspondingly. This ensemble model is able to identify 75% of the legitimate users correctly as well as 94% of the spammers. It classifies 68% of the users correctly as spammers and 96% correct rate for non-spammers, accordingly. We will save this trained model in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output\n",
    "\n",
    "We could utilize the [model persistence](http://scikit-learn.org/stable/modules/model_persistence.html) feature in scikit-learn to output our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ensemble_user_2.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputting all trained classifiers\n",
    "# (without the TFIDF version)\n",
    "# Caution: in order to output the correct model, please refer to the steps above and rerun them (the ones without the TFIDF feature)\n",
    "# so that the final pickle file captures the correct model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#joblib.dump(logreg, 'logreg_user.pkl')\n",
    "#joblib.dump(rf, 'rf_user.pkl') \n",
    "#joblib.dump(svm, 'svm_user.pkl')\n",
    "joblib.dump(ensemble, 'ensemble_user_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try unpack\n",
    "import pickle\n",
    "\n",
    "clf = joblib.load('ensemble_user.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrizeTrain55275\n",
      "[1]\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda\\envs\\py3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print(user.iloc[5047].screen_name)\n",
    "print(clf.predict(X_test.iloc[2].values.reshape(1, -1)))\n",
    "print(y_test.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-jupyter",
   "language": "python",
   "name": "py3-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
