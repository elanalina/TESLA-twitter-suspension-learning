{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Spam Learning: Classifer\n",
    "## CSCE 670 Spring 2018, Course Project\n",
    "### By: Rose Lin (826009602)\n",
    "\n",
    "There will be series of notebooks outlining how we train our models. This one looks into [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression), [random forest](https://en.wikipedia.org/wiki/Random_forest) and [SVM](https://en.wikipedia.org/wiki/Support_vector_machine) in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the aggregated user data\n",
    "\n",
    "Our main data sources come from the [Bot Repository](https://botometer.iuni.iu.edu/bot-repository/datasets.html):\n",
    "\n",
    "* Varol-2017, which was released in 2017. It contains 2,573 user IDs crawled in April 2016. We repeatedly called Twitter API to crawl account and tweet information. Despite some suspended accounts, we were able to get information of 2,401 users. \n",
    "* cresci-2017, annotated by CrowdFlower contributors. We downloded the whole dataset and used the following labels: genuine, social_spambots_1, social_spambots_2, social_spambots_3, traditional_spambots_1, traditional_spambots_2, traditional_spambots_3, and traditional_spambots_4.\n",
    "\n",
    "Initially, our aggregated user dataset was imbalanced. We had ~7,000 labeled spammers and ~5,000 labeled legitimate users. To balance it out, we did not use oversampling/downsampling; rather, we utilized Twitter API again. Our crawler started from President Trump's [Twitter account](https://twitter.com/realDonaldTrump) and scraped his friends lists, his friends' following lists, and so on until we collected 2,000 rows of user data. We assume that President Trump is following real users, and his friends follow authentic accounts as well. \n",
    "\n",
    "After the initial run, we noticed that our account based model did not perform well. That is because most data was old (except for President Trump's 2000+ records). Thus, we acquired more data using the Twitter API again. \n",
    "\n",
    "* We were able to acquire additional 9,573 spam accounts information using the streaming API. We filtered out tweet streams on 4/15 and 4/16/2018. We used the following keywords, with the assumption that whoever sent a tweet containing this keyword was a spam account: ['make money from home','enter to win','Credit Card', 'lonely', 'debt','deals','ad', '100% free','Act now','apply online','Click below','Click here', 'Extra cash','Offer expires', 'order now','Save $','Serious cash','Satisfaction guaranteed', 'Supplies are limited', 'trial','Work from home','you are a winner','your income','Weight loss','why pay more']. Sources of the spam keywords: [455 Spam Trigger Words to Avoid in 2018](https://prospect.io/blog/455-email-spam-trigger-words-avoid-2018/), [“SPAM Tweets” – 5 Buzzwords that Attract Spammers](http://www.adweek.com/digital/spam-tweets-5-buzzwords-that-attract-spammers/)\n",
    "\n",
    "* We also scraped 9,464 ham user data using the same assumption as the one for President Trump's above, but this time our initial seed is from Dr. [Philip Guo](https://twitter.com/pgbovine/), Assistant Professor of Cognitive Science at UC San Diego.\n",
    "\n",
    "In this way, we were able to gather a balanced user dataset with spammers:legitimate users ratio roughly to be 1:1 (15,731 spammers, 16,828 legitimate users). It is acknowledged that our aggregated dataset may subject to biases. If time permits, we will collect a larger dataset that covers as many groups as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the user data...\n",
      "Number of users: 32500\n",
      "Number of spammers: 15731\n",
      "Number of legitmate users: 16828\n"
     ]
    }
   ],
   "source": [
    "#Loading the aggregated data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import date\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(\"Loading the user data...\")\n",
    "user = pd.read_csv(cwd+\"/416.csv\",sep=',',header='infer',low_memory=False)\n",
    "print(\"Number of users:\",user.id.nunique())\n",
    "print(\"Number of spammers:\",len(user[user.user_type == '1']))\n",
    "print(\"Number of legitmate users:\",len(user[user.user_type == '0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors_enabled</th>\n",
       "      <th>crawled_at</th>\n",
       "      <th>created_at</th>\n",
       "      <th>default_profile</th>\n",
       "      <th>default_profile_image</th>\n",
       "      <th>description</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>follow_request_sent</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following</th>\n",
       "      <th>...</th>\n",
       "      <th>profile_text_color</th>\n",
       "      <th>profile_use_background_image</th>\n",
       "      <th>protected</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>time_zone</th>\n",
       "      <th>url</th>\n",
       "      <th>user_type</th>\n",
       "      <th>utc_offset</th>\n",
       "      <th>verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2014-04-19 14:46:19</td>\n",
       "      <td>Tue Mar 17 08:51:12 +0000 2009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>22</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>davideb66</td>\n",
       "      <td>1299</td>\n",
       "      <td>Rome</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-18 23:20:58</td>\n",
       "      <td>Sun Apr 19 14:38:04 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Autrice del libro #unavitatuttacurve dal 9 apr...</td>\n",
       "      <td>16358</td>\n",
       "      <td></td>\n",
       "      <td>12561</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>ElisaDospina</td>\n",
       "      <td>18665</td>\n",
       "      <td>Greenland</td>\n",
       "      <td>http://t.co/ceK8TovxwI</td>\n",
       "      <td>1</td>\n",
       "      <td>-7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-13 23:21:54</td>\n",
       "      <td>Wed May 13 15:34:41 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Live Long and Prosper]</td>\n",
       "      <td>14</td>\n",
       "      <td></td>\n",
       "      <td>600</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>Vladimir65</td>\n",
       "      <td>22987</td>\n",
       "      <td>Rome</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-19 23:24:18</td>\n",
       "      <td>Wed Jul 15 12:55:03 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Cuasi Odontologa*♥,#Bipolar, #Sarcastica &amp; Som...</td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>398</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>3E4415</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>RafielaMorales</td>\n",
       "      <td>7975</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>-25200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2014-05-11 23:22:23</td>\n",
       "      <td>Wed Aug 05 21:12:49 +0000 2009</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>I shall rise from my own death, to avenge hers...</td>\n",
       "      <td>162</td>\n",
       "      <td></td>\n",
       "      <td>413</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>D67345</td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td>FabrizioC_c</td>\n",
       "      <td>20218</td>\n",
       "      <td>Rome</td>\n",
       "      <td>http://t.co/PK5F0JDKcy</td>\n",
       "      <td>1</td>\n",
       "      <td>7200.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors_enabled           crawled_at                      created_at  \\\n",
       "0                       2014-04-19 14:46:19  Tue Mar 17 08:51:12 +0000 2009   \n",
       "1                       2014-05-18 23:20:58  Sun Apr 19 14:38:04 +0000 2009   \n",
       "2                       2014-05-13 23:21:54  Wed May 13 15:34:41 +0000 2009   \n",
       "3                       2014-05-19 23:24:18  Wed Jul 15 12:55:03 +0000 2009   \n",
       "4                       2014-05-11 23:22:23  Wed Aug 05 21:12:49 +0000 2009   \n",
       "\n",
       "  default_profile default_profile_image  \\\n",
       "0             1.0                   1.0   \n",
       "1                                         \n",
       "2                                         \n",
       "3                                         \n",
       "4                                         \n",
       "\n",
       "                                         description  favourites_count  \\\n",
       "0                                                                    1   \n",
       "1  Autrice del libro #unavitatuttacurve dal 9 apr...             16358   \n",
       "2                            [Live Long and Prosper]                14   \n",
       "3  Cuasi Odontologa*♥,#Bipolar, #Sarcastica & Som...                11   \n",
       "4  I shall rise from my own death, to avenge hers...               162   \n",
       "\n",
       "  follow_request_sent  followers_count following   ...     profile_text_color  \\\n",
       "0                                   22             ...                 333333   \n",
       "1                                12561             ...                 333333   \n",
       "2                                  600             ...                 333333   \n",
       "3                                  398             ...                 3E4415   \n",
       "4                                  413             ...                 D67345   \n",
       "\n",
       "  profile_use_background_image  protected     screen_name statuses_count  \\\n",
       "0                          1.0                  davideb66           1299   \n",
       "1                          1.0               ElisaDospina          18665   \n",
       "2                          1.0                 Vladimir65          22987   \n",
       "3                          1.0             RafielaMorales           7975   \n",
       "4                          1.0                FabrizioC_c          20218   \n",
       "\n",
       "                    time_zone                     url user_type utc_offset  \\\n",
       "0                        Rome                                 1     7200.0   \n",
       "1                   Greenland  http://t.co/ceK8TovxwI         1    -7200.0   \n",
       "2                        Rome                                 1     7200.0   \n",
       "3  Pacific Time (US & Canada)                                 1   -25200.0   \n",
       "4                        Rome  http://t.co/PK5F0JDKcy         1     7200.0   \n",
       "\n",
       "  verified  \n",
       "0           \n",
       "1           \n",
       "2           \n",
       "3           \n",
       "4           \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the head\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contributors_enabled                  object\n",
       "crawled_at                            object\n",
       "created_at                            object\n",
       "default_profile                       object\n",
       "default_profile_image                 object\n",
       "description                           object\n",
       "favourites_count                       int64\n",
       "follow_request_sent                   object\n",
       "followers_count                        int64\n",
       "following                             object\n",
       "friends_count                          int64\n",
       "geo_enabled                           object\n",
       "id                                     int64\n",
       "is_translator                         object\n",
       "lang                                  object\n",
       "listed_count                           int64\n",
       "location                              object\n",
       "name                                  object\n",
       "notifications                         object\n",
       "profile_background_color              object\n",
       "profile_background_image_url          object\n",
       "profile_background_image_url_https    object\n",
       "profile_background_tile               object\n",
       "profile_banner_url                    object\n",
       "profile_image_url                     object\n",
       "profile_image_url_https               object\n",
       "profile_link_color                    object\n",
       "profile_sidebar_border_color          object\n",
       "profile_sidebar_fill_color            object\n",
       "profile_text_color                    object\n",
       "profile_use_background_image          object\n",
       "protected                             object\n",
       "screen_name                           object\n",
       "statuses_count                         int64\n",
       "time_zone                             object\n",
       "url                                   object\n",
       "user_type                             object\n",
       "utc_offset                            object\n",
       "verified                              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to reset type here\n",
    "user.contributors_enabled = user.contributors_enabled.astype('category')\n",
    "user.default_profile = user.default_profile.astype('category')\n",
    "user.default_profile_image = user.default_profile_image.astype('category')\n",
    "user.geo_enabled = user.geo_enabled.astype('category')\n",
    "user.is_translator = user.is_translator.astype('category')\n",
    "user.profile_background_tile = user.profile_background_tile.astype('category')\n",
    "user.profile_use_background_image = user.profile_use_background_image.astype('category')\n",
    "user.user_type = pd.to_numeric(user.user_type, downcast='integer', errors='coerce')\n",
    "user.user_type = user.user_type.fillna(0.0).astype('int64')\n",
    "#user.crawled_at = user.crawled_at.astype('datetime64')\n",
    "#user.created_at = user.created_at.astype('datetime64')\n",
    "#user.favourites_count = user.favourites_count.astype('float64')\n",
    "#user.followers_count = user.followers_count.astype('float64')\n",
    "#user.friends_count = user.friends_count.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "contributors_enabled                  category\n",
       "crawled_at                              object\n",
       "created_at                              object\n",
       "default_profile                       category\n",
       "default_profile_image                 category\n",
       "description                             object\n",
       "favourites_count                         int64\n",
       "follow_request_sent                     object\n",
       "followers_count                          int64\n",
       "following                               object\n",
       "friends_count                            int64\n",
       "geo_enabled                           category\n",
       "id                                       int64\n",
       "is_translator                         category\n",
       "lang                                    object\n",
       "listed_count                             int64\n",
       "location                                object\n",
       "name                                    object\n",
       "notifications                           object\n",
       "profile_background_color                object\n",
       "profile_background_image_url            object\n",
       "profile_background_image_url_https      object\n",
       "profile_background_tile               category\n",
       "profile_banner_url                      object\n",
       "profile_image_url                       object\n",
       "profile_image_url_https                 object\n",
       "profile_link_color                      object\n",
       "profile_sidebar_border_color            object\n",
       "profile_sidebar_fill_color              object\n",
       "profile_text_color                      object\n",
       "profile_use_background_image          category\n",
       "protected                               object\n",
       "screen_name                             object\n",
       "statuses_count                           int64\n",
       "time_zone                               object\n",
       "url                                     object\n",
       "user_type                                int64\n",
       "utc_offset                              object\n",
       "verified                                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mainly converted the json response from Twitter API into the dataframe, with two additional features:\n",
    "* crawled_at: the date a record was crawled. It will be used for account age computation.\n",
    "* user_type: 0 = normal users, 1 = spammers. It serves as a binary indicator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Based on the visualization results, we will consider the following account features:\n",
    "\n",
    "* Count of favorite tweets\n",
    "* Friends to follower ratio\n",
    "* Total status count\n",
    "* Default profile image\n",
    "* Default profile\n",
    "* Account ages\n",
    "* Username, count of characters\n",
    "* Username, count of numbers\n",
    "* Screen_name, count of characters\n",
    "* Screen_name, count of numbers\n",
    "* Length of description \n",
    "* ~~Description text~~ (we tried TF-IDF, but the offline training corpus cannot accommodate all possible texts available in the description text. Moreover, TF-IDF disregards orders, and models with TF-IDF descrption text did not perform well - see the other notebook. Thus, we've decided to remove it)\n",
    "* ~~Average tweet per day~~ (eventually removed because it correlates with total status count and ages)\n",
    "\n",
    "Moreover, these account features are considered as well:\n",
    "\n",
    "* Listed count: The number of public lists that this user is a member of. Our guess is that spammers may not be on too many public lists, unless these lists were specifically designed for a given campaign.\n",
    "* Description, count of hashtags\n",
    "* Description, count of @ symbol\n",
    "* Description, count of URLs\n",
    "\n",
    "These features will be derived from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# Create a new dataframe to store the result\n",
    "usert = pd.DataFrame()\n",
    "# add count of favorite tweets\n",
    "usert['favorite_count'] = user['favourites_count']\n",
    "# add friends to follower ratio\n",
    "usert['friends_to_followers'] = user['friends_count'] / user['followers_count']\n",
    "# add total status count\n",
    "usert['statuses_count'] = user['statuses_count']\n",
    "# add default profile image\n",
    "temp_df = pd.get_dummies(user['default_profile_image'])\n",
    "temp_df.columns = ['def_p_img_na','def_p_img_false','def_p_img_true']\n",
    "usert = pd.concat([usert, temp_df], axis=1)\n",
    "# add default profile\n",
    "temp_df = pd.get_dummies(user['default_profile'])\n",
    "temp_df.columns = ['def_p_na','def_p_false','def_p_true']\n",
    "usert = pd.concat([usert, temp_df], axis=1)\n",
    "# add account ages \n",
    "agedf = pd.to_datetime(user['crawled_at'])-pd.to_datetime(user['created_at'])\n",
    "usert['age'] = agedf.dt.days\n",
    "# add username, count of characters and letters\n",
    "for index, item in user['name'].items():\n",
    "    letter = 0\n",
    "    num = 0\n",
    "    for c in item:\n",
    "        if c.isalpha():\n",
    "            letter += 1\n",
    "        elif c.isdigit():\n",
    "            num += 1\n",
    "    usert.loc[index,'name_letter'] = letter\n",
    "    usert.loc[index,'name_num'] = num\n",
    "# add screen name, count of characters and letters\n",
    "for index, item in user['screen_name'].items():\n",
    "    letter = 0\n",
    "    num = 0\n",
    "    for c in item:\n",
    "        if c.isalpha():\n",
    "            letter += 1\n",
    "        elif c.isdigit():\n",
    "            num += 1\n",
    "    usert.loc[index,'screen_letter'] = letter\n",
    "    usert.loc[index,'screen_num'] = num\n",
    "# add len of description\n",
    "usert['des_len'] = pd.Series([len(d) for d in user['description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add listed count\n",
    "usert['listed_count'] = user['listed_count']\n",
    "# add description related features\n",
    "for index, item in user['description'].items():\n",
    "    usert.loc[index,'bio_hashtag'] = item.count('#')\n",
    "    usert.loc[index,'bio_at'] = item.count('@')\n",
    "    usert.loc[index,'bio_url'] = item.count('http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no description text (TFIDF)\n",
    "#import re, string\n",
    "#from nltk.stem import PorterStemmer\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#tfidf_transformer = TfidfVectorizer()\n",
    "#des_text = tfidf_transformer.fit_transform(user['description'].tolist())\n",
    "#des_text\n",
    "# because the description text is tooooo large, we won't add it to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   favorite_count  friends_to_followers  statuses_count  def_p_img_na  \\\n",
      "0               1              1.818182            1299             0   \n",
      "1           16358              0.274023           18665             1   \n",
      "2              14              1.258333           22987             1   \n",
      "3              11              0.879397            7975             1   \n",
      "4             162              0.980630           20218             1   \n",
      "\n",
      "   def_p_img_false  def_p_img_true  def_p_na  def_p_false  def_p_true   age  \\\n",
      "0                0               1         0            0           1  1859   \n",
      "1                0               0         1            0           0  1855   \n",
      "2                0               0         1            0           0  1826   \n",
      "3                0               0         1            0           0  1769   \n",
      "4                0               0         1            0           0  1740   \n",
      "\n",
      "   name_letter  name_num  screen_letter  screen_num  des_len  listed_count  \\\n",
      "0         13.0       0.0            7.0         2.0        1             0   \n",
      "1         12.0       0.0           12.0         0.0      134           110   \n",
      "2         14.0       0.0            8.0         2.0       23             6   \n",
      "3         15.0       0.0           14.0         0.0      149             2   \n",
      "4          4.0       0.0           10.0         0.0       79             8   \n",
      "\n",
      "   bio_hashtag  bio_at  bio_url  \n",
      "0          0.0     0.0      0.0  \n",
      "1          3.0     2.0      0.0  \n",
      "2          0.0     0.0      0.0  \n",
      "3          3.0     0.0      0.0  \n",
      "4          0.0     0.0      0.0  \n",
      "\n",
      "       favorite_count  friends_to_followers  statuses_count  def_p_img_na  \\\n",
      "count    33687.000000          33687.000000    3.368700e+04  33687.000000   \n",
      "mean      6994.511355           1831.148822    2.064410e+04      0.295218   \n",
      "std      22399.560307          13398.213595    7.720248e+04      0.456147   \n",
      "min          0.000000              0.000000    0.000000e+00      0.000000   \n",
      "25%          3.000000              0.291432    3.600000e+02      0.000000   \n",
      "50%        531.000000              0.928962    3.531000e+03      0.000000   \n",
      "75%       4345.000000              2.282398    1.547800e+04      1.000000   \n",
      "max     657248.000000         100000.000000    8.250670e+06      1.000000   \n",
      "\n",
      "       def_p_img_false  def_p_img_true      def_p_na   def_p_false  \\\n",
      "count     33687.000000    33687.000000  33687.000000  33687.000000   \n",
      "mean          0.697569        0.007213      0.253510      0.418945   \n",
      "std           0.459318        0.084626      0.435027      0.493394   \n",
      "min           0.000000        0.000000      0.000000      0.000000   \n",
      "25%           0.000000        0.000000      0.000000      0.000000   \n",
      "50%           1.000000        0.000000      0.000000      0.000000   \n",
      "75%           1.000000        0.000000      1.000000      1.000000   \n",
      "max           1.000000        1.000000      1.000000      1.000000   \n",
      "\n",
      "         def_p_true           age   name_letter      name_num  screen_letter  \\\n",
      "count  33687.000000  33687.000000  33687.000000  33687.000000   33687.000000   \n",
      "mean       0.327545   1714.254312     11.101642      0.053552      10.300591   \n",
      "std        0.469325   1235.333933      4.274657      0.421453       2.950854   \n",
      "min        0.000000     -2.000000      0.000000      0.000000       0.000000   \n",
      "25%        0.000000    511.000000      9.000000      0.000000       8.000000   \n",
      "50%        0.000000   1697.000000     11.000000      0.000000      11.000000   \n",
      "75%        1.000000   2788.000000     14.000000      0.000000      13.000000   \n",
      "max        1.000000   4408.000000     45.000000     14.000000      15.000000   \n",
      "\n",
      "         screen_num       des_len   listed_count   bio_hashtag        bio_at  \\\n",
      "count  33687.000000  33687.000000   33687.000000  33687.000000  33687.000000   \n",
      "mean       0.450411     74.265830     709.422507      0.284501      0.367234   \n",
      "std        1.233415     57.084016    6254.081250      1.130679      0.923834   \n",
      "min        0.000000      1.000000       0.000000      0.000000      0.000000   \n",
      "25%        0.000000     15.000000       0.000000      0.000000      0.000000   \n",
      "50%        0.000000     73.000000       8.000000      0.000000      0.000000   \n",
      "75%        0.000000    128.000000      80.500000      0.000000      0.000000   \n",
      "max       14.000000    343.000000  228892.000000     20.000000     11.000000   \n",
      "\n",
      "            bio_url  \n",
      "count  33687.000000  \n",
      "mean       0.090807  \n",
      "std        0.346791  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        0.000000  \n",
      "75%        0.000000  \n",
      "max        7.000000  \n"
     ]
    }
   ],
   "source": [
    "# add average tweet per day\n",
    "#usert['avg_tweet_per_day'] = usert['statuses_count']/usert['age']\n",
    "# Now let's look at the new dataframe!\n",
    "print(usert.head())\n",
    "print(\"\")\n",
    "print(usert.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we have all the desired features now, we still need to do some final checks so that our classifiers can process these data without any question. We are mainly concern about 1) duplicates, and 2) missing values.\n",
    "\n",
    "## Analysis on duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 duplicated records in total.\n"
     ]
    }
   ],
   "source": [
    "# identify duplicate rows in the original dataframe\n",
    "count = 0\n",
    "for index, row in user.duplicated().items():\n",
    "    if row is True:\n",
    "        print(index, row)\n",
    "        count += 1\n",
    "print(\"There are\",count,\"duplicated records in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are free from duplicates now! How about missing values?\n",
    "\n",
    "## Analysis on missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At column favorite_count # of NA records: 0\n",
      "At column friends_to_followers # of NA records: 381\n",
      "At column statuses_count # of NA records: 0\n",
      "At column def_p_img_na # of NA records: 0\n",
      "At column def_p_img_false # of NA records: 0\n",
      "At column def_p_img_true # of NA records: 0\n",
      "At column def_p_na # of NA records: 0\n",
      "At column def_p_false # of NA records: 0\n",
      "At column def_p_true # of NA records: 0\n",
      "At column age # of NA records: 0\n",
      "At column name_letter # of NA records: 0\n",
      "At column name_num # of NA records: 0\n",
      "At column screen_letter # of NA records: 0\n",
      "At column screen_num # of NA records: 0\n",
      "At column des_len # of NA records: 0\n",
      "At column listed_count # of NA records: 0\n",
      "At column bio_hashtag # of NA records: 0\n",
      "At column bio_at # of NA records: 0\n",
      "At column bio_url # of NA records: 0\n"
     ]
    }
   ],
   "source": [
    "# Check to see if there is any NA\n",
    "for c in usert.columns.values:\n",
    "    print(\"At column\",c,\"# of NA records:\",usert[c].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all other columns are good except for the *friends_to_followers* ratio column that contains some NA. Let's see what these records are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465196345 0 0\n",
      "465306140 0 0\n",
      "465318952 0 0\n",
      "465320112 0 0\n",
      "465325633 0 0\n",
      "465328572 0 0\n",
      "465335657 0 0\n",
      "465338328 0 0\n",
      "465343577 0 0\n",
      "465349136 0 0\n",
      "465360460 0 0\n",
      "465366176 0 0\n",
      "465369079 0 0\n",
      "465369276 0 0\n",
      "465371415 0 0\n",
      "465373317 0 0\n",
      "465373507 0 0\n",
      "465373538 0 0\n",
      "465376231 0 0\n",
      "465376509 0 0\n",
      "465378609 0 0\n",
      "465379325 0 0\n",
      "465387088 0 0\n",
      "465387671 0 0\n",
      "465392182 0 0\n",
      "465393290 0 0\n",
      "465398499 0 0\n",
      "465418896 0 0\n",
      "465427410 0 0\n",
      "465428761 0 0\n",
      "465434130 0 0\n",
      "465434388 0 0\n",
      "465435993 0 0\n",
      "465442625 0 0\n",
      "465445130 0 0\n",
      "466109264 0 0\n",
      "466114317 0 0\n",
      "466116893 0 0\n",
      "466121357 0 0\n",
      "466124818 0 0\n",
      "466125372 0 0\n",
      "466126074 0 0\n",
      "466143639 0 0\n",
      "466152441 0 0\n",
      "466154098 0 0\n",
      "466155797 0 0\n",
      "466163583 0 0\n",
      "466175265 0 0\n",
      "466182757 0 0\n",
      "466183322 0 0\n",
      "466184623 0 0\n",
      "466188621 0 0\n",
      "466189042 0 0\n",
      "466189857 0 0\n",
      "466192045 0 0\n",
      "466192470 0 0\n",
      "466194674 0 0\n",
      "466195745 0 0\n",
      "466200874 0 0\n",
      "466205550 0 0\n",
      "466212113 0 0\n",
      "466217028 0 0\n",
      "466217564 0 0\n",
      "466220361 0 0\n",
      "466225850 0 0\n",
      "466226701 0 0\n",
      "466226882 0 0\n",
      "466227486 0 0\n",
      "466229029 0 0\n",
      "466232314 0 0\n",
      "466234508 0 0\n",
      "466235245 0 0\n",
      "466237339 0 0\n",
      "466238747 0 0\n",
      "466239539 0 0\n",
      "466241419 0 0\n",
      "466246135 0 0\n",
      "466247056 0 0\n",
      "466249274 0 0\n",
      "466256821 0 0\n",
      "466257207 0 0\n",
      "466257969 0 0\n",
      "466259699 0 0\n",
      "466264439 0 0\n",
      "466266821 0 0\n",
      "466278877 0 0\n",
      "466283003 0 0\n",
      "466289763 0 0\n",
      "466291288 0 0\n",
      "466292895 0 0\n",
      "466294714 0 0\n",
      "466295605 0 0\n",
      "466298694 0 0\n",
      "466300701 0 0\n",
      "466300896 0 0\n",
      "466303061 0 0\n",
      "466304146 0 0\n",
      "466305983 0 0\n",
      "466308638 0 0\n",
      "466308751 0 0\n",
      "466309651 0 0\n",
      "466310732 0 0\n",
      "466322512 0 0\n",
      "466324791 0 0\n",
      "466326143 0 0\n",
      "466328816 0 0\n",
      "466329548 0 0\n",
      "466330859 0 0\n",
      "466332009 0 0\n",
      "466334990 0 0\n",
      "466335221 0 0\n",
      "466343368 0 0\n",
      "466352701 0 0\n",
      "466357272 0 0\n",
      "466358135 0 0\n",
      "466359810 0 0\n",
      "466362716 0 0\n",
      "466363084 0 0\n",
      "466370213 0 0\n",
      "466371617 0 0\n",
      "466372053 0 0\n",
      "466375110 0 0\n",
      "466376085 0 0\n",
      "466376780 0 0\n",
      "466379768 0 0\n",
      "466381987 0 0\n",
      "466383527 0 0\n",
      "466384281 0 0\n",
      "466386773 0 0\n",
      "466387081 0 0\n",
      "466388338 0 0\n",
      "466394064 0 0\n",
      "466394269 0 0\n",
      "466395997 0 0\n",
      "466398478 0 0\n",
      "466400012 0 0\n",
      "466400825 0 0\n",
      "466401624 0 0\n",
      "466406088 0 0\n",
      "466409574 0 0\n",
      "466413855 0 0\n",
      "466415308 0 0\n",
      "466426767 0 0\n",
      "466429038 0 0\n",
      "466429651 0 0\n",
      "466430387 0 0\n",
      "466432166 0 0\n",
      "466444765 0 0\n",
      "466462018 0 0\n",
      "466462817 0 0\n",
      "466463170 0 0\n",
      "466463528 0 0\n",
      "466472496 0 0\n",
      "466473505 0 0\n",
      "466475273 0 0\n",
      "466475858 0 0\n",
      "466479806 0 0\n",
      "466480215 0 0\n",
      "467056238 0 0\n",
      "467059649 0 0\n",
      "467060651 0 0\n",
      "467060783 0 0\n",
      "467061366 0 0\n",
      "467065641 0 0\n",
      "467067671 0 0\n",
      "467069697 0 0\n",
      "467077394 0 0\n",
      "467079499 0 0\n",
      "467082134 0 0\n",
      "467084434 0 0\n",
      "467092868 0 0\n",
      "467102366 0 0\n",
      "467106023 0 0\n",
      "467109178 0 0\n",
      "467110763 0 0\n",
      "467117750 0 0\n",
      "467119810 0 0\n",
      "467123168 0 0\n",
      "467123525 0 0\n",
      "467124165 0 0\n",
      "467124670 0 0\n",
      "467126150 0 0\n",
      "467126825 0 0\n",
      "467128482 0 0\n",
      "467131009 0 0\n",
      "467133470 0 0\n",
      "467135461 0 0\n",
      "467135856 0 0\n",
      "467136996 0 0\n",
      "467142582 0 0\n",
      "467143029 0 0\n",
      "467144458 0 0\n",
      "467145195 0 0\n",
      "467147087 0 0\n",
      "467149516 0 0\n",
      "467152720 0 0\n",
      "467153239 0 0\n",
      "467153925 0 0\n",
      "467156950 0 0\n",
      "467157688 0 0\n",
      "467157882 0 0\n",
      "467162901 0 0\n",
      "467164236 0 0\n",
      "467176923 0 0\n",
      "467178690 0 0\n",
      "467185497 0 0\n",
      "467186118 0 0\n",
      "467187664 0 0\n",
      "467189683 0 0\n",
      "467191312 0 0\n",
      "467193516 0 0\n",
      "467194119 0 0\n",
      "467194846 0 0\n",
      "467199549 0 0\n",
      "467200089 0 0\n",
      "467200465 0 0\n",
      "467201459 0 0\n",
      "467203615 0 0\n",
      "467203990 0 0\n",
      "467999325 0 0\n",
      "468004034 0 0\n",
      "468014280 0 0\n",
      "468016342 0 0\n",
      "468022909 0 0\n",
      "468025547 0 0\n",
      "468029740 0 0\n",
      "468033321 0 0\n",
      "468036974 0 0\n",
      "468037917 0 0\n",
      "468048856 0 0\n",
      "468055750 0 0\n",
      "468058184 0 0\n",
      "468059646 0 0\n",
      "468062676 0 0\n",
      "468064243 0 0\n",
      "468068306 0 0\n",
      "468071828 0 0\n",
      "468073544 0 0\n",
      "468080210 0 0\n",
      "468083973 0 0\n",
      "468099552 0 0\n",
      "468101494 0 0\n",
      "468102975 0 0\n",
      "468126635 0 0\n",
      "468129657 0 0\n",
      "468133931 0 0\n",
      "468141676 0 0\n",
      "468143141 0 0\n",
      "468144332 0 0\n",
      "468145003 0 0\n",
      "468148455 0 0\n",
      "468155641 0 0\n",
      "538861240 0 0\n",
      "538865597 0 0\n",
      "538877096 0 0\n",
      "538879598 0 0\n",
      "538883948 0 0\n",
      "538887557 0 0\n",
      "538905719 0 0\n",
      "538907144 0 0\n",
      "538910117 0 0\n",
      "538911422 0 0\n",
      "538916096 0 0\n",
      "538927099 0 0\n",
      "538945998 0 0\n",
      "538952010 0 0\n",
      "538989284 0 0\n",
      "538990279 0 0\n",
      "539038007 0 0\n",
      "539045903 0 0\n",
      "539048096 0 0\n",
      "539052577 0 0\n",
      "539060340 0 0\n",
      "539087929 0 0\n",
      "539091058 0 0\n",
      "539094006 0 0\n",
      "539096910 0 0\n",
      "539103877 0 0\n",
      "539109831 0 0\n",
      "539110886 0 0\n",
      "539130256 0 0\n",
      "539150535 0 0\n",
      "539157095 0 0\n",
      "539186544 0 0\n",
      "539219286 0 0\n",
      "539272485 0 0\n",
      "539276634 0 0\n",
      "542129791 0 0\n",
      "542135879 0 0\n",
      "542150978 0 0\n",
      "542152604 0 0\n",
      "542163896 0 0\n",
      "542215293 0 0\n",
      "542216247 0 0\n",
      "542219577 0 0\n",
      "542236806 0 0\n",
      "545207749 0 0\n",
      "545306050 0 0\n",
      "545308370 0 0\n",
      "545309342 0 0\n",
      "545309765 0 0\n",
      "545708233 0 0\n",
      "545708584 0 0\n",
      "2351877541 0 0\n",
      "2351708202 0 0\n",
      "179562837 0 0\n",
      "179295032 0 0\n",
      "3078705434 0 0\n",
      "15488734 0 0\n",
      "56839754 0 0\n",
      "56840437 0 0\n",
      "56872455 0 0\n",
      "57351732 0 0\n",
      "56872758 0 0\n",
      "57097440 0 0\n",
      "57082889 0 0\n",
      "57097621 0 0\n",
      "57351026 0 0\n",
      "3069252070 0 0\n",
      "56210449 0 0\n",
      "3078501312 0 0\n",
      "55926935 0 0\n",
      "55927230 0 0\n",
      "55927062 0 0\n",
      "55926879 0 0\n",
      "55926996 0 0\n",
      "56250124 0 0\n",
      "56158032 0 0\n",
      "3302525926 0 0\n",
      "3287882455 0 0\n",
      "3400714546 0 0\n",
      "950399726792327170 0 0\n",
      "984984544645443585 0 0\n",
      "985728515122151424 0 0\n",
      "983893320157073408 0 0\n",
      "984818074279964678 0 0\n",
      "984396963721265157 0 0\n",
      "863696332363575296 0 0\n",
      "985856523988602881 0 0\n",
      "981722331302199296 0 0\n",
      "971920526934331394 0 0\n",
      "971921697661046784 0 0\n",
      "971572913315856385 0 0\n",
      "971923860474937344 0 0\n",
      "971924616942821379 0 0\n",
      "979629628670316544 0 0\n",
      "971926603902013440 0 0\n",
      "971928303450783744 0 0\n",
      "971932098377338881 0 0\n",
      "971933163004903424 0 0\n",
      "984549474180661248 0 0\n",
      "984765488109707271 0 0\n",
      "971934546164826113 0 0\n",
      "971935724512600064 0 0\n",
      "971201083186995200 0 0\n",
      "971937104467935232 0 0\n",
      "971938434531995648 0 0\n",
      "971940262380371969 0 0\n",
      "971940933112483842 0 0\n",
      "971942460539547650 0 0\n",
      "971944707088465921 0 0\n",
      "971945784169005056 0 0\n",
      "979630348354146304 0 0\n",
      "443515349 0 0\n",
      "979981997412990978 0 0\n",
      "725420430 0 0\n",
      "984446293853356033 0 0\n",
      "983351837565874176 0 0\n",
      "984226870341357568 0 0\n",
      "985863466681622528 0 0\n",
      "981128714783199233 0 0\n",
      "411166063 0 0\n",
      "959237277506453504 0 0\n",
      "985870738732154881 0 0\n",
      "969519355561107456 0 0\n",
      "918806040329666561 0 0\n",
      "985856927933476864 0 0\n",
      "565229453 0 0\n",
      "979632842861891584 0 0\n",
      "3439063295 0 0\n",
      "984984544645443585 0 0\n"
     ]
    }
   ],
   "source": [
    "# Extract the indexes of row that are nan\n",
    "nan_index = [index for index, row in usert.friends_to_followers.items() if np.isnan(row)]\n",
    "for i in nan_index:\n",
    "    record = user.iloc[i]\n",
    "    print(record.id, record.friends_count, record.followers_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like most of the nan comes from dividing 0.\n",
    "\n",
    "To properly handle this issue, we would set the ratio to be **100000** if the followers_count is 0 (not infinity because the classifier can't handle infinities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At column friends_to_followers, # of NA records: 0\n"
     ]
    }
   ],
   "source": [
    "# Fixing the friends_to_followers ratio\n",
    "# This way is slower but hopefully more accurate\n",
    "for index, row in user.iterrows():\n",
    "    if row['followers_count'] == 0:\n",
    "        usert.loc[index,'friends_to_followers'] = 100000\n",
    "\n",
    "# Check if there is still any NA\n",
    "print(\"At column friends_to_followers, # of NA records:\",usert['friends_to_followers'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terrific! Now we are free from NAs :) We can proceed to the next step: training classifiers!\n",
    "\n",
    "Note: we remove *avg_tweet_per_day* here and save the remaining features into another dataframe as it correlates with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'usert' (DataFrame)\n",
      "Stored 'user' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# to store the data\n",
    "%store usert\n",
    "%store user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to restore data\n",
    "%store -r usert\n",
    "%store -r user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Classifier: training\n",
    "\n",
    "We will split the whole dataset into 80% training and 20% testing.\n",
    "\n",
    "Given that we have more observations than features, one may argue that we should normalize our data first. Nonetheless, our trained model will be used in online prediction. We have not came up with a way of normalizing input data under the online setting. Thus, we won't perform any transformation on our data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26949, 19) (26949,)\n",
      "(6738, 19) (6738,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(usert, user.user_type, test_size=0.2, random_state=0)\n",
    "#des_text_train, des_text_test = train_test_split(des_text, test_size=0.2, random_state=0)\n",
    "\n",
    "# check the size\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "#print(des_text_train.shape, des_text_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# New feature set - with TFIDF!\n",
    "#X_train_new = np.hstack((X_train,des_text_train.toarray()))\n",
    "#X_test_new = np.hstack((X_test,des_text_test.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.75      0.76      3419\n",
      "          1       0.75      0.75      0.75      3319\n",
      "\n",
      "avg / total       0.75      0.75      0.75      6738\n",
      "\n",
      "Logistic Regression accuracy: 0.7542297417631345\n",
      "ROC score: 0.7542284705191669\n"
     ]
    }
   ],
   "source": [
    "# run the actual classifier\n",
    "# without descrption text\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initial version: use default setting\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "pred = logreg.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"Logistic Regression report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Logistic Regression accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with text\n",
    "#logreg.fit(X_train_new, y_train)\n",
    "#pred = logreg.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "#print(\"Logistic Regression report:\")\n",
    "#print(classification_report(y_test, pred))\n",
    "#print(\"Logistic Regression accuracy:\", accuracy_score(y_test, pred))\n",
    "#print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like adding description text TFIDF increases the number of features, thus causing an overfitting issue.\n",
    "\n",
    "Next, we will try Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.91      0.89      3419\n",
      "          1       0.90      0.87      0.89      3319\n",
      "\n",
      "avg / total       0.89      0.89      0.89      6738\n",
      "\n",
      "Random Forest accuracy: 0.8901751261501929\n",
      "ROC score: 0.8899482457221801\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"Random Forest report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Random Forest accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how about w/ text features?\n",
    "#rf.fit(X_train_new, y_train)\n",
    "#pred = rf.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "#print(\"Random Forest report:\")\n",
    "#print(classification_report(y_test, pred))\n",
    "#print(\"Random Forest accuracy:\", accuracy_score(y_test, pred))\n",
    "#print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, random forest suffers when the description text is added. How about SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      1.00      0.74      3419\n",
      "          1       1.00      0.29      0.45      3319\n",
      "\n",
      "avg / total       0.79      0.65      0.60      6738\n",
      "\n",
      "SVM accuracy: 0.649450875630751\n",
      "ROC score: 0.6441699307020187\n"
     ]
    }
   ],
   "source": [
    "# No model adjustment version\n",
    "from sklearn import svm\n",
    "\n",
    "svm = svm.SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "pred = svm.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"SVM report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"SVM accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.04000000e+02 2.91666667e+00 1.96000000e+02 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [9.00000000e+01 5.82877960e-02 7.98000000e+02 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.05000000e+02 9.76627713e-01 4.05900000e+03 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " ...\n",
      " [4.68900000e+03 2.96624088e+00 1.48010000e+04 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.34780000e+04 7.14845839e-01 1.18595000e+05 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [1.08000000e+02 2.55000000e+00 1.42000000e+03 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]]\n",
      "[    1     2     3 ... 26943 26944 26947]\n",
      "[13221 11279]\n"
     ]
    }
   ],
   "source": [
    "# get support vectors\n",
    "print(svm.support_vectors_)\n",
    "\n",
    "# get indices of support vectors\n",
    "print(svm.support_) \n",
    "\n",
    "# get number of support vectors for each class\n",
    "print(svm.n_support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how about w/ text features?\n",
    "#svm.fit(X_train_new, y_train)\n",
    "#pred = svm.predict(X_test_new)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "#print(\"SVM report:\")\n",
    "#print(classification_report(y_test, pred))\n",
    "#print(\"SVM accuracy:\", accuracy_score(y_test, pred))\n",
    "#print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our initial thought was to use LASSO to scale down the size of total features available (especially with TFIDF included). Nonetheless, it seems that LASSO is a variation of the generalized linear model and thus not applicable for this project (we are doing classification instead of regression). Thus, we won't explore further.\n",
    "\n",
    "Below is a summary of model performance: (average reported for precision, recall and F1-score)\n",
    "\n",
    "| Model               | Accuracy | Precision | Recall | F1-Score | ROC Score |\n",
    "|---------------------|----------|-----------|--------|----------|-----------|\n",
    "| Logistic Regression | 0.7542   | 0.75      | 0.75   | 0.74     | 0.7542    |\n",
    "| Random Forest       | 0.8901   | 0.89      | 0.89   | 0.89     | 0.8899    |\n",
    "| SVM                 | 0.6494   | 0.79      | 0.65   | 0.60     | 0.6441    |\n",
    "\n",
    "Next, we will attempt to combine the models together and consider output it for our website.\n",
    "\n",
    "## Model Fine-tuning\n",
    "\n",
    "Based on our observations, Random Forest seems to perform the best at this case. We will fine tuning this model with the help from [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html).\n",
    "\n",
    "Parameters for consideration: (thanks to the guide [here](http://scikit-learn.org/stable/modules/ensemble.html#parameters)!)\n",
    "\n",
    "* n_estimators: number of trees in the forest, default is 10. Per discussions online, the number of tree really depends upon the feature size. Some say that the number of trees should be roughly equivalent to sqrt(# of features), so in our case that will be sqrt(19) ~ 4. Nonetheless, in general the accuracy gets better with more trees in the forest. Technically, we can run as many trees as we want, since RF suffers little from the overfitting problem. As the number of trees in the forest increases, so as the training time. So we won't be too agressive here, but would like to try a small range up to 1000.\n",
    "* max_features: the size of the random subsets of features to consider when splitting a node. Since we are doing a classification here, we will go with sqrt(# of features) (which is the default one we used before). But we also would like to set this parameter to (# of features) and see what will happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.8982893613863223\n",
      "{'max_features': 'auto', 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# source: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators':[4,10,50,100,500,1000], 'max_features':['auto',None]}\n",
    "rf = RandomForestClassifier()\n",
    "clf = GridSearchCV(rf, parameters,n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_score_)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.89      0.90      3419\n",
      "          1       0.89      0.91      0.90      3319\n",
      "\n",
      "avg / total       0.90      0.90      0.90      6738\n",
      "\n",
      "Random Forest accuracy: 0.8999703176016622\n",
      "ROC score: 0.9000980906990436\n"
     ]
    }
   ],
   "source": [
    "# Using the best selection above\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "rf.fit(X_train, y_train)\n",
    "pred = rf.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"Random Forest report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Random Forest accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 0 - favorite_count (0.186155)\n",
      "2. feature 9 - age (0.147245)\n",
      "3. feature 2 - statuses_count (0.133973)\n",
      "4. feature 1 - friends_to_followers (0.123106)\n",
      "5. feature 15 - listed_count (0.123038)\n",
      "6. feature 14 - des_len (0.057516)\n",
      "7. feature 10 - name_letter (0.036934)\n",
      "8. feature 3 - def_p_img_na (0.033069)\n",
      "9. feature 4 - def_p_img_false (0.032241)\n",
      "10. feature 12 - screen_letter (0.027669)\n",
      "11. feature 17 - bio_at (0.019359)\n",
      "12. feature 13 - screen_num (0.018893)\n",
      "13. feature 6 - def_p_na (0.016700)\n",
      "14. feature 7 - def_p_false (0.014606)\n",
      "15. feature 16 - bio_hashtag (0.010442)\n",
      "16. feature 8 - def_p_true (0.010082)\n",
      "17. feature 18 - bio_url (0.005701)\n",
      "18. feature 11 - name_num (0.002472)\n",
      "19. feature 5 - def_p_img_true (0.000800)\n"
     ]
    }
   ],
   "source": [
    "# Also print out feature importance\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. feature %d - %s (%f)\" % (f + 1, indices[f], usert.columns.values[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, seems like RF really improves... a little bit. Another [post](http://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees) mentions about extremely randomized trees. Will that help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Extremely Randomized Trees report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.89      0.89      3419\n",
      "          1       0.89      0.88      0.88      3319\n",
      "\n",
      "avg / total       0.88      0.88      0.88      6738\n",
      "\n",
      " Extremely Randomized Trees accuracy: 0.8839418224992579\n",
      "ROC score: 0.8838545670336821\n"
     ]
    }
   ],
   "source": [
    "# Extremely randomized tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "ext = ExtraTreesClassifier(n_estimators=1000)\n",
    "ext.fit(X_train, y_train)\n",
    "pred = ext.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"Extremely Randomized Trees report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Extremely Randomized Trees accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like randomly splitting features without considering the most discriminative thresholds helps out a little here. We could have tuned this model further but eventually decided not to. \n",
    "\n",
    "When using a prior dataset, we found that kNN achieves an accuracy of 93%. Does that still hold true here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.78      0.77      3419\n",
      "          1       0.77      0.76      0.77      3319\n",
      "\n",
      "avg / total       0.77      0.77      0.77      6738\n",
      "\n",
      "KNN accuracy: 0.7699614128821609\n",
      "ROC score: 0.7698182030640499\n"
     ]
    }
   ],
   "source": [
    "# Checking KNN - using default parameters\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "pred = knn.predict(X_test)\n",
    "\n",
    "# Getting some evaluation metrics here\n",
    "print(\"KNN report:\")\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"KNN accuracy:\", accuracy_score(y_test, pred))\n",
    "print(\"ROC score:\",roc_auc_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well.. KNN is on par with logistics, so we won't consider it further here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model output\n",
    "\n",
    "We could utilize the [model persistence](http://scikit-learn.org/stable/modules/model_persistence.html) feature in scikit-learn to output our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_user_2.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputting all trained classifiers\n",
    "# (without the TFIDF version)\n",
    "# Caution: in order to output the correct model, please refer to the steps above and rerun them (the ones without the TFIDF feature)\n",
    "# so that the final pickle file captures the correct model\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#joblib.dump(logreg, 'logreg_user.pkl')\n",
    "#joblib.dump(rf, 'rf_user.pkl') \n",
    "#joblib.dump(svm, 'svm_user.pkl')\n",
    "joblib.dump(rf, 'rf_user_2.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-jupyter",
   "language": "python",
   "name": "py3-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
